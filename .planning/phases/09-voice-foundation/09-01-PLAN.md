---
phase: 09-voice-foundation
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - package.json
  - src/main/voice/wakeWordDetector.js
  - src/main/voice/whisperClient.js
  - src/main/ipc/voiceHandlers.js
  - src/_boot.js
  - src/classes/audioCapture.class.js
  - src/classes/voiceController.class.js
files_created:
  - src/main/voice/wakeWordDetector.js
  - src/main/voice/whisperClient.js
  - src/main/ipc/voiceHandlers.js
  - src/classes/audioCapture.class.js
  - src/classes/voiceController.class.js
  - resources/wake-word/son-of-anton.ppn
autonomous: false
user_setup:
  - service: picovoice
    why: "Wake word detection requires Porcupine SDK"
    env_vars:
      - name: PICOVOICE_ACCESS_KEY
        source: "https://console.picovoice.ai/ -> Dashboard -> AccessKey"
    dashboard_config:
      - task: "Create free account"
        location: "https://console.picovoice.ai/"
      - task: "Train 'Son of Anton' wake word model"
        location: "Console -> Porcupine -> Train -> 'Son of Anton' -> Download .ppn"

must_haves:
  truths:
    - "Wake word 'Son of Anton' detected with Porcupine SDK"
    - "Microphone permission requested and obtained"
    - "Audio frames streamed from renderer to main process via IPC"
    - "Whisper API transcribes captured audio"
    - "Voice state machine transitions: IDLE -> LISTENING -> RECORDING -> PROCESSING -> IDLE"
    - "60 second maximum recording duration enforced"
    - "Space key cancels listening mode immediately"
  artifacts:
    - path: "src/main/voice/wakeWordDetector.js"
      provides: "Porcupine integration for wake word detection"
      contains: "porcupine.process"
    - path: "src/main/voice/whisperClient.js"
      provides: "OpenAI Whisper API integration"
      contains: "openai.audio.transcriptions.create"
    - path: "src/main/ipc/voiceHandlers.js"
      provides: "IPC bridge between renderer and main process"
      contains: "ipcMain.handle"
    - path: "src/classes/audioCapture.class.js"
      provides: "Microphone capture with frame extraction"
      contains: "navigator.mediaDevices.getUserMedia"
    - path: "src/classes/voiceController.class.js"
      provides: "Voice state machine orchestration"
      contains: "class VoiceController"
  key_links:
    - from: "audioCapture.class.js"
      to: "voiceHandlers.js"
      via: "IPC voice:audio-frame"
      pattern: "send.*voice:audio-frame"
    - from: "wakeWordDetector.js"
      to: "renderer"
      via: "IPC voice:wake-word-detected"
      pattern: "sender\\.send.*voice:wake-word-detected"
    - from: "voiceController.class.js"
      to: "whisperClient.js"
      via: "IPC voice:transcribe"
      pattern: "invoke.*voice:transcribe"
---

<objective>
Implement core voice infrastructure: wake word detection with Porcupine, speech-to-text with Whisper API, and voice state machine.

Purpose: Establish the foundation for voice control by creating the Porcupine/Whisper pipeline, IPC communication layer, and state management. This enables Phase 09-02 to focus purely on UX polish.

Output: Working voice pipeline where "Son of Anton" triggers recording, audio is captured, and transcription is returned (not yet displayed in terminal).
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/09-voice-foundation/09-RESEARCH.md
@.planning/phases/09-voice-foundation/09-CONTEXT.md
</context>

<tasks>

<task type="checkpoint:human-action" gate="blocking">
  <name>Task 0: User Setup - Picovoice Account and Model</name>
  <files>resources/wake-word/son-of-anton.ppn, .env</files>
  <action>
**User must complete these steps before execution can proceed:**

1. **Create Picovoice Account (FREE)**
   - Navigate to https://console.picovoice.ai/
   - Sign up with email or GitHub (free, no credit card)
   - Copy AccessKey from dashboard

2. **Train Wake Word Model**
   - Go to Porcupine page in console
   - Select language: English
   - Type phrase: `Son of Anton`
   - Select platform: Windows (or your OS)
   - Click "Train" (takes ~10 seconds)
   - Download the .ppn file

3. **Add Files to Project**
   ```bash
   mkdir -p resources/wake-word
   # Copy downloaded son-of-anton_en_windows_v3_0_0.ppn to:
   # resources/wake-word/son-of-anton.ppn
   ```

4. **Configure Environment**
   ```bash
   # Add to .env file:
   PICOVOICE_ACCESS_KEY=your-access-key-here
   # Verify OPENAI_API_KEY exists (already used by project)
   ```
  </action>
  <verify>
1. File exists: `resources/wake-word/son-of-anton.ppn`
2. `.env` contains `PICOVOICE_ACCESS_KEY=...`
3. `.env` contains `OPENAI_API_KEY=...`
  </verify>
  <done>Picovoice account created, wake word model trained and downloaded, environment configured</done>
  <resume-signal>Type "done" when setup is complete</resume-signal>
</task>

<task type="auto">
  <name>Task 1: Install Porcupine Node.js SDK</name>
  <files>package.json</files>
  <action>
Install Porcupine Node.js SDK for wake word detection:

```bash
npm install @picovoice/porcupine-node
```

**Note:** openai package already installed (4.10.0 pinned for Node 16 compatibility).

Verify package.json includes the dependency after installation.
  </action>
  <verify>
Run: `npm ls @picovoice/porcupine-node`
Expected: Shows installed version (^3.0.0 or similar)
  </verify>
  <done>Porcupine Node.js SDK installed in package.json</done>
</task>

<task type="auto">
  <name>Task 2: Create Wake Word Detector (Main Process)</name>
  <files>src/main/voice/wakeWordDetector.js</files>
  <action>
Create `src/main/voice/wakeWordDetector.js`:

```javascript
/**
 * Wake Word Detector using Picovoice Porcupine
 * Runs in Electron main process
 */

const { Porcupine } = require('@picovoice/porcupine-node');

class WakeWordDetector {
  constructor(accessKey, modelPath) {
    this.accessKey = accessKey;
    this.modelPath = modelPath;
    this.porcupine = null;
    this.isInitialized = false;
  }

  /**
   * Initialize Porcupine with the wake word model
   * @returns {Object} Frame length and sample rate requirements
   */
  initialize() {
    if (this.isInitialized) {
      return this.getAudioRequirements();
    }

    try {
      this.porcupine = new Porcupine(
        this.accessKey,
        [this.modelPath],
        [0.5] // sensitivity: 0-1, higher = more sensitive but more false positives
      );
      this.isInitialized = true;
      console.log('[Porcupine] Initialized successfully');
      console.log('[Porcupine] Frame length:', this.porcupine.frameLength);
      console.log('[Porcupine] Sample rate:', this.porcupine.sampleRate);
      return this.getAudioRequirements();
    } catch (error) {
      console.error('[Porcupine] Initialization failed:', error.message);
      throw error;
    }
  }

  /**
   * Get audio requirements for frame processing
   */
  getAudioRequirements() {
    return {
      frameLength: this.porcupine?.frameLength ?? 512,
      sampleRate: this.porcupine?.sampleRate ?? 16000,
    };
  }

  /**
   * Process a single audio frame
   * @param {Int16Array} frame - Audio frame (512 samples, 16kHz, signed 16-bit)
   * @returns {boolean} True if wake word detected
   */
  processFrame(frame) {
    if (!this.isInitialized || !this.porcupine) {
      return false;
    }

    try {
      const keywordIndex = this.porcupine.process(frame);
      if (keywordIndex >= 0) {
        console.log('[Porcupine] Wake word detected!');
        return true;
      }
      return false;
    } catch (error) {
      console.warn('[Porcupine] Frame processing error:', error.message);
      return false;
    }
  }

  /**
   * Release Porcupine resources
   */
  release() {
    if (this.porcupine) {
      this.porcupine.release();
      this.porcupine = null;
      this.isInitialized = false;
      console.log('[Porcupine] Released');
    }
  }
}

module.exports = { WakeWordDetector };
```

Create directory if needed: `mkdir -p src/main/voice`
  </action>
  <verify>
1. File exists: `src/main/voice/wakeWordDetector.js`
2. Run: `node -e "require('./src/main/voice/wakeWordDetector.js')"` - should not error
  </verify>
  <done>WakeWordDetector class created with Porcupine integration</done>
</task>

<task type="auto">
  <name>Task 3: Create Whisper Client (Main Process)</name>
  <files>src/main/voice/whisperClient.js</files>
  <action>
Create `src/main/voice/whisperClient.js`:

```javascript
/**
 * Whisper API Client for speech-to-text
 * Runs in Electron main process
 */

const OpenAI = require('openai');
const fs = require('fs');
const path = require('path');
const os = require('os');

class WhisperClient {
  constructor(apiKey) {
    this.openai = new OpenAI({ apiKey });
  }

  /**
   * Transcribe audio buffer to text
   * @param {Buffer} audioBuffer - Audio data (WebM/Opus format)
   * @returns {Promise<string>} Transcription text
   */
  async transcribe(audioBuffer) {
    if (!audioBuffer || audioBuffer.length === 0) {
      console.warn('[Whisper] Empty audio buffer received');
      return '';
    }

    // Write to temp file (Whisper API requires file upload)
    const tempPath = path.join(os.tmpdir(), `voice-${Date.now()}.webm`);

    try {
      fs.writeFileSync(tempPath, audioBuffer);
      console.log('[Whisper] Transcribing audio:', audioBuffer.length, 'bytes');

      const transcription = await this.openai.audio.transcriptions.create({
        file: fs.createReadStream(tempPath),
        model: 'whisper-1',
        language: 'en',
      });

      console.log('[Whisper] Transcription:', transcription.text);
      return transcription.text;
    } catch (error) {
      console.error('[Whisper] Transcription failed:', error.message);
      throw error;
    } finally {
      // Cleanup temp file
      try {
        if (fs.existsSync(tempPath)) {
          fs.unlinkSync(tempPath);
        }
      } catch (cleanupError) {
        console.warn('[Whisper] Temp file cleanup failed:', cleanupError.message);
      }
    }
  }
}

module.exports = { WhisperClient };
```
  </action>
  <verify>
1. File exists: `src/main/voice/whisperClient.js`
2. Run: `node -e "require('./src/main/voice/whisperClient.js')"` - should not error
  </verify>
  <done>WhisperClient class created with OpenAI API integration</done>
</task>

<task type="auto">
  <name>Task 4: Create Voice IPC Handlers (Main Process)</name>
  <files>src/main/ipc/voiceHandlers.js</files>
  <action>
Create `src/main/ipc/voiceHandlers.js`:

```javascript
/**
 * Voice IPC Handlers
 * Bridge between renderer audio capture and main process voice processing
 */

const { ipcMain } = require('electron');
const path = require('path');
const fs = require('fs');
const { WakeWordDetector } = require('../voice/wakeWordDetector');
const { WhisperClient } = require('../voice/whisperClient');

let wakeWordDetector = null;
let whisperClient = null;
let mainWindow = null;

/**
 * Setup voice IPC handlers
 * @param {BrowserWindow} window - Main Electron window
 */
function setupVoiceIPC(window) {
  mainWindow = window;

  // Check voice availability
  ipcMain.handle('voice:check-availability', () => {
    const accessKey = process.env.PICOVOICE_ACCESS_KEY;
    const openaiKey = process.env.OPENAI_API_KEY;
    const modelPath = path.join(__dirname, '../../..', 'resources/wake-word/son-of-anton.ppn');
    const modelExists = fs.existsSync(modelPath);

    return {
      available: !!(accessKey && openaiKey && modelExists),
      hasAccessKey: !!accessKey,
      hasOpenAIKey: !!openaiKey,
      hasModel: modelExists,
    };
  });

  // Initialize voice services
  ipcMain.handle('voice:initialize', async () => {
    try {
      const accessKey = process.env.PICOVOICE_ACCESS_KEY;
      const openaiKey = process.env.OPENAI_API_KEY;

      if (!accessKey) {
        throw new Error('PICOVOICE_ACCESS_KEY not configured');
      }
      if (!openaiKey) {
        throw new Error('OPENAI_API_KEY not configured');
      }

      // Determine model path
      const modelPath = path.join(__dirname, '../../..', 'resources/wake-word/son-of-anton.ppn');
      if (!fs.existsSync(modelPath)) {
        throw new Error(`Wake word model not found at: ${modelPath}`);
      }

      wakeWordDetector = new WakeWordDetector(accessKey, modelPath);
      whisperClient = new WhisperClient(openaiKey);

      const audioReqs = wakeWordDetector.initialize();
      console.log('[VoiceIPC] Voice services initialized');

      return {
        success: true,
        ...audioReqs,
      };
    } catch (error) {
      console.error('[VoiceIPC] Initialization failed:', error.message);
      return {
        success: false,
        error: error.message,
      };
    }
  });

  // Process audio frame for wake word detection
  ipcMain.on('voice:audio-frame', (event, frameData) => {
    if (!wakeWordDetector) return;

    // Convert from regular array to Int16Array if needed
    const frame = frameData instanceof Int16Array
      ? frameData
      : new Int16Array(frameData);

    const detected = wakeWordDetector.processFrame(frame);
    if (detected) {
      event.sender.send('voice:wake-word-detected');
    }
  });

  // Transcribe audio with Whisper
  ipcMain.handle('voice:transcribe', async (event, audioData) => {
    if (!whisperClient) {
      return { success: false, error: 'Whisper client not initialized' };
    }

    try {
      // Convert from regular array to Buffer if needed
      const audioBuffer = Buffer.isBuffer(audioData)
        ? audioData
        : Buffer.from(audioData);

      const text = await whisperClient.transcribe(audioBuffer);
      return { success: true, text };
    } catch (error) {
      console.error('[VoiceIPC] Transcription failed:', error.message);
      return { success: false, error: error.message };
    }
  });

  // Release voice services
  ipcMain.on('voice:release', () => {
    if (wakeWordDetector) {
      wakeWordDetector.release();
      wakeWordDetector = null;
    }
    whisperClient = null;
    console.log('[VoiceIPC] Voice services released');
  });
}

/**
 * Cleanup voice IPC handlers
 */
function cleanupVoiceIPC() {
  if (wakeWordDetector) {
    wakeWordDetector.release();
    wakeWordDetector = null;
  }
  whisperClient = null;
}

module.exports = { setupVoiceIPC, cleanupVoiceIPC };
```

Create directory if needed: `mkdir -p src/main/ipc`
  </action>
  <verify>
1. File exists: `src/main/ipc/voiceHandlers.js`
2. Run: `node -e "require('./src/main/ipc/voiceHandlers.js')"` - should not error
  </verify>
  <done>Voice IPC handlers created for main-renderer communication</done>
</task>

<task type="auto">
  <name>Task 5: Integrate Voice IPC into Boot Process</name>
  <files>src/_boot.js</files>
  <action>
Modify `src/_boot.js` to initialize voice IPC handlers.

**1. Add require at top of file (after other requires around line 40):**
```javascript
const { setupVoiceIPC, cleanupVoiceIPC } = require('./main/ipc/voiceHandlers');
```

**2. Add voice IPC setup after claudeStateManager initialization (around line 227):**
Find this block:
```javascript
win.webContents.on('did-finish-load', () => {
    claudeStateManager = new ClaudeStateManager(win);
    claudeStateManager.start();
    signale.success("Claude state manager initialized");
});
```

Add after `claudeStateManager.start()`:
```javascript
    // Initialize voice IPC handlers
    setupVoiceIPC(win);
    signale.success("Voice IPC handlers initialized");
```

**3. Add cleanup on app quit (add before or after the uncaughtException handler at top):**
```javascript
app.on('will-quit', () => {
    cleanupVoiceIPC();
});
```
  </action>
  <verify>
1. `grep -n "setupVoiceIPC" src/_boot.js` - should show import and call
2. `grep -n "cleanupVoiceIPC" src/_boot.js` - should show cleanup
  </verify>
  <done>Voice IPC integrated into Electron boot process</done>
</task>

<task type="auto">
  <name>Task 6: Create Audio Capture Class (Renderer)</name>
  <files>src/classes/audioCapture.class.js</files>
  <action>
Create `src/classes/audioCapture.class.js`:

```javascript
/**
 * Audio Capture Service
 * Handles microphone access and audio frame extraction for wake word detection
 */

class AudioCapture {
  constructor() {
    this.audioContext = null;
    this.mediaStream = null;
    this.processor = null;
    this.mediaRecorder = null;
    this.audioChunks = [];
    this.isCapturing = false;
    this.onAudioFrame = null;
  }

  /**
   * Request microphone permission
   * @returns {Promise<boolean>} True if permission granted
   */
  async requestPermission() {
    try {
      this.mediaStream = await navigator.mediaDevices.getUserMedia({
        audio: {
          sampleRate: 16000,
          channelCount: 1,
          echoCancellation: true,
          noiseSuppression: true,
        },
      });
      console.log('[AudioCapture] Microphone permission granted');
      return true;
    } catch (error) {
      console.error('[AudioCapture] Microphone permission denied:', error.message);
      return false;
    }
  }

  /**
   * Check if microphone is available
   */
  hasPermission() {
    return this.mediaStream !== null;
  }

  /**
   * Start capturing audio frames for wake word detection
   * @param {Function} onFrame - Callback receiving Int16Array frames
   */
  startFrameCapture(onFrame) {
    if (!this.mediaStream) {
      console.error('[AudioCapture] No media stream available');
      return false;
    }

    this.onAudioFrame = onFrame;
    this.audioContext = new AudioContext({ sampleRate: 16000 });
    const source = this.audioContext.createMediaStreamSource(this.mediaStream);

    // Process in 512-sample frames for Porcupine
    this.processor = this.audioContext.createScriptProcessor(512, 1, 1);
    this.processor.onaudioprocess = (event) => {
      if (!this.onAudioFrame) return;

      const inputData = event.inputBuffer.getChannelData(0);
      const frame = this._float32ToInt16(inputData);
      this.onAudioFrame(frame);
    };

    source.connect(this.processor);
    this.processor.connect(this.audioContext.destination);
    this.isCapturing = true;

    console.log('[AudioCapture] Frame capture started');
    return true;
  }

  /**
   * Convert Float32 audio samples to Int16 for Porcupine
   * @private
   */
  _float32ToInt16(float32Array) {
    const int16Array = new Int16Array(float32Array.length);
    for (let i = 0; i < float32Array.length; i++) {
      const s = Math.max(-1, Math.min(1, float32Array[i]));
      int16Array[i] = s < 0 ? s * 0x8000 : s * 0x7FFF;
    }
    return int16Array;
  }

  /**
   * Get current audio level (0-1) for visualization
   * @returns {number} RMS audio level
   */
  getAudioLevel() {
    if (!this.analyser || !this.dataArray) return 0;
    this.analyser.getByteTimeDomainData(this.dataArray);
    let sum = 0;
    for (let i = 0; i < this.dataArray.length; i++) {
      const normalized = (this.dataArray[i] - 128) / 128;
      sum += normalized * normalized;
    }
    return Math.sqrt(sum / this.dataArray.length);
  }

  /**
   * Setup analyser for audio visualization
   */
  setupAnalyser() {
    if (!this.audioContext || !this.mediaStream) return;

    this.analyser = this.audioContext.createAnalyser();
    this.analyser.fftSize = 256;
    this.dataArray = new Uint8Array(this.analyser.frequencyBinCount);

    const source = this.audioContext.createMediaStreamSource(this.mediaStream);
    source.connect(this.analyser);
  }

  /**
   * Start recording audio for Whisper transcription
   */
  startRecording() {
    if (!this.mediaStream) {
      console.error('[AudioCapture] No media stream for recording');
      return false;
    }

    this.audioChunks = [];
    this.mediaRecorder = new MediaRecorder(this.mediaStream, {
      mimeType: 'audio/webm;codecs=opus',
    });

    this.mediaRecorder.ondataavailable = (event) => {
      if (event.data.size > 0) {
        this.audioChunks.push(event.data);
      }
    };

    this.mediaRecorder.start(100); // Collect in 100ms chunks
    console.log('[AudioCapture] Recording started');
    return true;
  }

  /**
   * Stop recording and get audio blob
   * @returns {Promise<Blob>} Audio blob in WebM format
   */
  stopRecording() {
    return new Promise((resolve) => {
      if (!this.mediaRecorder || this.mediaRecorder.state === 'inactive') {
        resolve(new Blob([]));
        return;
      }

      this.mediaRecorder.onstop = () => {
        const blob = new Blob(this.audioChunks, { type: 'audio/webm' });
        console.log('[AudioCapture] Recording stopped:', blob.size, 'bytes');
        resolve(blob);
      };

      this.mediaRecorder.stop();
    });
  }

  /**
   * Stop frame capture
   */
  stopFrameCapture() {
    if (this.processor) {
      this.processor.disconnect();
      this.processor = null;
    }
    if (this.audioContext) {
      this.audioContext.close();
      this.audioContext = null;
    }
    this.onAudioFrame = null;
    this.isCapturing = false;
    console.log('[AudioCapture] Frame capture stopped');
  }

  /**
   * Release all resources
   */
  release() {
    this.stopFrameCapture();
    if (this.mediaStream) {
      this.mediaStream.getTracks().forEach(track => track.stop());
      this.mediaStream = null;
    }
    console.log('[AudioCapture] Released');
  }
}

module.exports = { AudioCapture };
```
  </action>
  <verify>
1. File exists: `src/classes/audioCapture.class.js`
  </verify>
  <done>AudioCapture class created for microphone access and frame extraction</done>
</task>

<task type="auto">
  <name>Task 7: Create Voice Controller Class (Renderer)</name>
  <files>src/classes/voiceController.class.js</files>
  <action>
Create `src/classes/voiceController.class.js`:

```javascript
/**
 * Voice Controller
 * Orchestrates wake word detection, recording, and transcription
 *
 * Context decisions implemented:
 * - 60 second maximum recording duration
 * - Space key cancels listening mode
 * - Wake word ignored during recording (re-triggering disabled)
 */

const { AudioCapture } = require('./audioCapture.class');

// Voice states
const VoiceState = {
  DISABLED: 'disabled',   // Voice not available or not initialized
  IDLE: 'idle',           // Ready but not listening
  LISTENING: 'listening', // Listening for wake word
  RECORDING: 'recording', // Recording audio after wake word
  PROCESSING: 'processing', // Whisper API processing
  ERROR: 'error',         // Error state
};

class VoiceController {
  constructor(options = {}) {
    // Max duration: 60 seconds (from CONTEXT.md)
    this.maxRecordingMs = options.maxRecordingMs || 60000;
    this.silenceTimeoutMs = options.silenceTimeoutMs || 2000;

    // Callbacks
    this.onStateChange = options.onStateChange || (() => {});
    this.onTranscription = options.onTranscription || (() => {});
    this.onError = options.onError || (() => {});
    this.onWakeDetected = options.onWakeDetected || (() => {});
    this.onAudioLevel = options.onAudioLevel || (() => {}); // For waveform viz

    this.audioCapture = new AudioCapture();

    this.state = VoiceState.DISABLED;
    this.silenceTimer = null;
    this.maxDurationTimer = null;
    this.audioLevelInterval = null;
    this.isInitialized = false;
    this.isEnabled = false; // Voice toggle state

    // Bind space key handler
    this._boundKeyHandler = this._handleKeyDown.bind(this);
  }

  /**
   * Initialize voice controller
   * @returns {Promise<boolean>} True if initialization successful
   */
  async initialize() {
    try {
      // Check voice availability via IPC
      const availability = await window.ipc.invoke('voice:check-availability');
      if (!availability.available) {
        console.warn('[VoiceController] Voice not available:', availability);
        this._setState(VoiceState.DISABLED);
        return false;
      }

      // Request microphone permission
      const hasPermission = await this.audioCapture.requestPermission();
      if (!hasPermission) {
        this.onError('Microphone permission denied');
        this._setState(VoiceState.ERROR);
        return false;
      }

      // Initialize voice services in main process
      const result = await window.ipc.invoke('voice:initialize');
      if (!result.success) {
        this.onError(result.error || 'Voice initialization failed');
        this._setState(VoiceState.ERROR);
        return false;
      }

      // Setup wake word detection listener
      window.ipc.on('voice:wake-word-detected', () => {
        this._onWakeWordDetected();
      });

      // Add global key handler for space key cancel
      document.addEventListener('keydown', this._boundKeyHandler);

      this.isInitialized = true;
      this._setState(VoiceState.IDLE);
      console.log('[VoiceController] Initialized');
      return true;
    } catch (error) {
      console.error('[VoiceController] Initialization failed:', error.message);
      this.onError(error.message);
      this._setState(VoiceState.ERROR);
      return false;
    }
  }

  /**
   * Handle keydown events - Space cancels recording
   * @private
   */
  _handleKeyDown(event) {
    // Space key cancels listening/recording mode
    if (event.code === 'Space' && (this.state === VoiceState.LISTENING || this.state === VoiceState.RECORDING)) {
      event.preventDefault();
      console.log('[VoiceController] Space key pressed - cancelling');
      this.cancelRecording();
    }
  }

  /**
   * Enable voice listening (toggle on)
   */
  enable() {
    if (!this.isInitialized) {
      console.warn('[VoiceController] Not initialized, cannot enable');
      return false;
    }
    this.isEnabled = true;
    return this.startListening();
  }

  /**
   * Disable voice listening (toggle off)
   */
  disable() {
    this.isEnabled = false;
    this.stopListening();
    this._setState(VoiceState.IDLE);
  }

  /**
   * Toggle voice on/off
   * @returns {boolean} New enabled state
   */
  toggle() {
    if (this.isEnabled) {
      this.disable();
    } else {
      this.enable();
    }
    return this.isEnabled;
  }

  /**
   * Start listening for wake word
   */
  startListening() {
    if (!this.isInitialized) {
      console.warn('[VoiceController] Not initialized');
      return false;
    }

    if (!this.isEnabled) {
      console.warn('[VoiceController] Voice is disabled');
      return false;
    }

    if (this.state === VoiceState.RECORDING || this.state === VoiceState.PROCESSING) {
      console.warn('[VoiceController] Cannot start listening from state:', this.state);
      return false;
    }

    // Start sending audio frames to main process for wake word detection
    this.audioCapture.startFrameCapture((frame) => {
      // Convert Int16Array to regular array for IPC
      window.ipc.send('voice:audio-frame', Array.from(frame));
    });

    this._setState(VoiceState.LISTENING);
    console.log('[VoiceController] Listening for wake word...');
    return true;
  }

  /**
   * Stop listening (but don't disable)
   */
  stopListening() {
    this._clearTimers();
    this._stopAudioLevelPolling();
    this.audioCapture.stopFrameCapture();

    if (this.state === VoiceState.RECORDING) {
      this.audioCapture.stopRecording();
    }

    if (this.isEnabled && this.isInitialized) {
      this._setState(VoiceState.LISTENING);
      // Restart listening for wake word
      this.audioCapture.startFrameCapture((frame) => {
        window.ipc.send('voice:audio-frame', Array.from(frame));
      });
    } else {
      this._setState(VoiceState.IDLE);
    }

    console.log('[VoiceController] Stopped listening');
  }

  /**
   * Cancel current recording without transcribing
   */
  cancelRecording() {
    console.log('[VoiceController] Recording cancelled');
    this._clearTimers();
    this._stopAudioLevelPolling();

    if (this.state === VoiceState.RECORDING) {
      this.audioCapture.stopRecording(); // Discard audio
    }

    // Return to listening if enabled
    if (this.isEnabled) {
      this._setState(VoiceState.LISTENING);
      this.audioCapture.startFrameCapture((frame) => {
        window.ipc.send('voice:audio-frame', Array.from(frame));
      });
    } else {
      this._setState(VoiceState.IDLE);
    }
  }

  /**
   * Handle wake word detection
   * @private
   */
  _onWakeWordDetected() {
    // Ignore wake word during recording (per CONTEXT.md)
    if (this.state !== VoiceState.LISTENING) {
      return;
    }

    console.log('[VoiceController] Wake word detected!');

    // Notify listeners (for audio feedback)
    this.onWakeDetected();

    // Start recording for transcription
    this._setState(VoiceState.RECORDING);
    this.audioCapture.startRecording();

    // Setup analyser for waveform visualization
    this.audioCapture.setupAnalyser();
    this._startAudioLevelPolling();

    // Start silence timeout (user can still speak)
    this._startSilenceTimer();

    // Start max duration timer (60 seconds per CONTEXT.md)
    this._startMaxDurationTimer();
  }

  /**
   * Start polling audio levels for waveform visualization
   * @private
   */
  _startAudioLevelPolling() {
    this._stopAudioLevelPolling();
    this.audioLevelInterval = setInterval(() => {
      const level = this.audioCapture.getAudioLevel();
      this.onAudioLevel(level);
    }, 50); // 20fps
  }

  /**
   * Stop audio level polling
   * @private
   */
  _stopAudioLevelPolling() {
    if (this.audioLevelInterval) {
      clearInterval(this.audioLevelInterval);
      this.audioLevelInterval = null;
    }
  }

  /**
   * Start silence timeout
   * @private
   */
  _startSilenceTimer() {
    this._clearSilenceTimer();
    this.silenceTimer = setTimeout(() => {
      this._onSilenceTimeout();
    }, this.silenceTimeoutMs);
  }

  /**
   * Clear silence timeout
   * @private
   */
  _clearSilenceTimer() {
    if (this.silenceTimer) {
      clearTimeout(this.silenceTimer);
      this.silenceTimer = null;
    }
  }

  /**
   * Start max duration timer (60 seconds)
   * @private
   */
  _startMaxDurationTimer() {
    this._clearMaxDurationTimer();
    this.maxDurationTimer = setTimeout(() => {
      console.log('[VoiceController] Max duration reached (60s)');
      this._onSilenceTimeout(); // Same behavior as silence timeout
    }, this.maxRecordingMs);
  }

  /**
   * Clear max duration timer
   * @private
   */
  _clearMaxDurationTimer() {
    if (this.maxDurationTimer) {
      clearTimeout(this.maxDurationTimer);
      this.maxDurationTimer = null;
    }
  }

  /**
   * Clear all timers
   * @private
   */
  _clearTimers() {
    this._clearSilenceTimer();
    this._clearMaxDurationTimer();
  }

  /**
   * Handle silence/max duration timeout - stop recording and transcribe
   * @private
   */
  async _onSilenceTimeout() {
    if (this.state !== VoiceState.RECORDING) {
      return;
    }

    console.log('[VoiceController] Timeout, processing...');
    this._clearTimers();
    this._stopAudioLevelPolling();
    this._setState(VoiceState.PROCESSING);

    try {
      // Stop recording and get audio
      const audioBlob = await this.audioCapture.stopRecording();

      if (audioBlob.size === 0) {
        console.warn('[VoiceController] No audio captured');
        this.onError('No audio captured');
        this._returnToListening();
        return;
      }

      // Convert blob to array buffer for IPC
      const arrayBuffer = await audioBlob.arrayBuffer();
      const audioData = Array.from(new Uint8Array(arrayBuffer));

      // Send to Whisper for transcription
      const result = await window.ipc.invoke('voice:transcribe', audioData);

      if (result.success && result.text) {
        console.log('[VoiceController] Transcription:', result.text);
        this.onTranscription(result.text, true); // true = success
      } else {
        console.warn('[VoiceController] Transcription failed:', result.error);
        this.onTranscription(null, false); // false = failure
        this.onError(result.error || 'Transcription failed');
      }
    } catch (error) {
      console.error('[VoiceController] Processing error:', error.message);
      this.onTranscription(null, false);
      this.onError(error.message);
    }

    this._returnToListening();
  }

  /**
   * Return to listening state if enabled
   * @private
   */
  _returnToListening() {
    if (this.isEnabled) {
      this._setState(VoiceState.LISTENING);
      this.audioCapture.startFrameCapture((frame) => {
        window.ipc.send('voice:audio-frame', Array.from(frame));
      });
    } else {
      this._setState(VoiceState.IDLE);
    }
  }

  /**
   * Update state and notify listeners
   * @private
   */
  _setState(newState) {
    const oldState = this.state;
    this.state = newState;
    if (oldState !== newState) {
      console.log('[VoiceController] State:', oldState, '->', newState);
      this.onStateChange(newState, oldState);
    }
  }

  /**
   * Get current state
   */
  getState() {
    return this.state;
  }

  /**
   * Check if voice is enabled
   */
  getEnabled() {
    return this.isEnabled;
  }

  /**
   * Release all resources
   */
  release() {
    this._clearTimers();
    this._stopAudioLevelPolling();
    document.removeEventListener('keydown', this._boundKeyHandler);
    this.audioCapture.release();
    window.ipc.send('voice:release');
    this._setState(VoiceState.DISABLED);
    this.isInitialized = false;
    this.isEnabled = false;
    console.log('[VoiceController] Released');
  }
}

module.exports = { VoiceController, VoiceState };
```
  </action>
  <verify>
1. File exists: `src/classes/voiceController.class.js`
2. Contains `maxRecordingMs` for 60s timeout
3. Contains `_handleKeyDown` for space key cancel
  </verify>
  <done>VoiceController class created with state machine, 60s timeout, and space key cancel</done>
</task>

</tasks>

<verification>
After all tasks complete:

1. **Dependencies installed:**
   ```bash
   npm ls @picovoice/porcupine-node
   ```

2. **Files created:**
   - `src/main/voice/wakeWordDetector.js`
   - `src/main/voice/whisperClient.js`
   - `src/main/ipc/voiceHandlers.js`
   - `src/classes/audioCapture.class.js`
   - `src/classes/voiceController.class.js`
   - `resources/wake-word/son-of-anton.ppn` (user created)

3. **Boot integration:**
   ```bash
   grep -n "setupVoiceIPC" src/_boot.js
   ```

4. **State machine states:**
   - DISABLED, IDLE, LISTENING, RECORDING, PROCESSING, ERROR

5. **Context decisions implemented:**
   - 60 second max recording: `maxRecordingMs: 60000`
   - Space key cancel: `_handleKeyDown` with `event.code === 'Space'`
   - Wake word ignored during recording: check in `_onWakeWordDetected`
</verification>

<success_criteria>
1. Porcupine SDK installed and importable
2. WakeWordDetector processes audio frames
3. WhisperClient transcribes audio via API
4. IPC handlers bridge renderer and main process
5. VoiceController state machine manages flow
6. 60-second max duration timer implemented
7. Space key cancels recording
8. Microphone permission requested
</success_criteria>

<output>
After completion, create `.planning/phases/09-voice-foundation/09-01-SUMMARY.md`
</output>
