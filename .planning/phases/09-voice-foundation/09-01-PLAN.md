---
phase: 09-voice-foundation
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - package.json
  - src/main/voice/wakeWordDetector.js
  - src/main/voice/whisperClient.js
  - src/main/ipc/voiceHandlers.js
files_created:
  - src/main/voice/wakeWordDetector.js
  - src/main/voice/whisperClient.js
  - src/main/ipc/voiceHandlers.js
  - resources/wake-word/son-of-anton.ppn
autonomous: false
user_setup:
  - service: picovoice
    why: "Wake word detection requires Porcupine SDK"
    env_vars:
      - name: PICOVOICE_ACCESS_KEY
        source: "https://console.picovoice.ai/ -> Dashboard -> AccessKey"
    dashboard_config:
      - task: "Create free account"
        location: "https://console.picovoice.ai/"
      - task: "Train 'Son of Anton' wake word model"
        location: "Console -> Porcupine -> Train -> 'Son of Anton' -> Download .ppn"

must_haves:
  truths:
    - "User can say 'Son of Anton' and app detects it"
    - "User's speech is transcribed to text via cloud API"
    - "Voice features work when configured, show helpful error when not"
  artifacts:
    - path: "src/main/voice/wakeWordDetector.js"
      provides: "Porcupine integration for wake word detection"
      contains: "porcupine.process"
    - path: "src/main/voice/whisperClient.js"
      provides: "OpenAI Whisper API integration"
      contains: "openai.audio.transcriptions.create"
    - path: "src/main/ipc/voiceHandlers.js"
      provides: "IPC bridge between renderer and main process"
      contains: "ipcMain.handle"
  key_links:
    - from: "voiceHandlers.js"
      to: "wakeWordDetector.js"
      via: "instantiation and frame processing"
      pattern: "WakeWordDetector"
    - from: "voiceHandlers.js"
      to: "whisperClient.js"
      via: "transcription invocation"
      pattern: "whisperClient.transcribe"
---

<objective>
Create voice infrastructure modules: wake word detector, Whisper transcription client, and IPC handlers.

Purpose: Establish the main-process voice components that Plan 02 will wire into the application. These modules are standalone and testable without renderer integration.

Output: Three main-process modules ready for boot integration in Plan 02.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/09-voice-foundation/09-RESEARCH.md
@.planning/phases/09-voice-foundation/09-CONTEXT.md
</context>

<tasks>

<task type="checkpoint:human-action" gate="blocking">
  <name>Task 0: User Setup - Picovoice Account and Model</name>
  <files>resources/wake-word/son-of-anton.ppn, .env</files>
  <action>
**User must complete these steps before execution can proceed:**

1. **Create Picovoice Account (FREE)**
   - Navigate to https://console.picovoice.ai/
   - Sign up with email or GitHub (free, no credit card)
   - Copy AccessKey from dashboard

2. **Train Wake Word Model**
   - Go to Porcupine page in console
   - Select language: English
   - Type phrase: `Son of Anton`
   - Select platform: Windows (or your OS)
   - Click "Train" (takes ~10 seconds)
   - Download the .ppn file

3. **Add Files to Project**
   ```bash
   mkdir -p resources/wake-word
   # Copy downloaded son-of-anton_en_windows_v3_0_0.ppn to:
   # resources/wake-word/son-of-anton.ppn
   ```

4. **Configure Environment**
   ```bash
   # Add to .env file:
   PICOVOICE_ACCESS_KEY=your-access-key-here
   # Verify OPENAI_API_KEY exists (already used by project)
   ```
  </action>
  <verify>
1. File exists: `resources/wake-word/son-of-anton.ppn`
2. `.env` contains `PICOVOICE_ACCESS_KEY=...`
3. `.env` contains `OPENAI_API_KEY=...`
  </verify>
  <done>Picovoice account created, wake word model trained and downloaded, environment configured</done>
  <resume-signal>Type "done" when setup is complete</resume-signal>
</task>

<task type="auto">
  <name>Task 1: Install Porcupine Node.js SDK</name>
  <files>package.json</files>
  <action>
Install Porcupine Node.js SDK for wake word detection:

```bash
npm install @picovoice/porcupine-node
```

**Note:** openai package already installed (4.10.0 pinned for Node 16 compatibility).

Verify package.json includes the dependency after installation.
  </action>
  <verify>
Run: `npm ls @picovoice/porcupine-node`
Expected: Shows installed version (^3.0.0 or similar)
  </verify>
  <done>Porcupine Node.js SDK installed in package.json</done>
</task>

<task type="auto">
  <name>Task 2: Create Wake Word Detector and Whisper Client</name>
  <files>src/main/voice/wakeWordDetector.js, src/main/voice/whisperClient.js</files>
  <action>
Create directory: `mkdir -p src/main/voice`

**Create `src/main/voice/wakeWordDetector.js`:**

```javascript
/**
 * Wake Word Detector using Picovoice Porcupine
 * Runs in Electron main process
 */

const { Porcupine } = require('@picovoice/porcupine-node');

class WakeWordDetector {
  constructor(accessKey, modelPath) {
    this.accessKey = accessKey;
    this.modelPath = modelPath;
    this.porcupine = null;
    this.isInitialized = false;
  }

  /**
   * Initialize Porcupine with the wake word model
   * @returns {Object} Frame length and sample rate requirements
   */
  initialize() {
    if (this.isInitialized) {
      return this.getAudioRequirements();
    }

    try {
      this.porcupine = new Porcupine(
        this.accessKey,
        [this.modelPath],
        [0.5] // sensitivity: 0-1, higher = more sensitive but more false positives
      );
      this.isInitialized = true;
      console.log('[Porcupine] Initialized successfully');
      console.log('[Porcupine] Frame length:', this.porcupine.frameLength);
      console.log('[Porcupine] Sample rate:', this.porcupine.sampleRate);
      return this.getAudioRequirements();
    } catch (error) {
      console.error('[Porcupine] Initialization failed:', error.message);
      throw error;
    }
  }

  /**
   * Get audio requirements for frame processing
   */
  getAudioRequirements() {
    return {
      frameLength: this.porcupine?.frameLength ?? 512,
      sampleRate: this.porcupine?.sampleRate ?? 16000,
    };
  }

  /**
   * Process a single audio frame
   * @param {Int16Array} frame - Audio frame (512 samples, 16kHz, signed 16-bit)
   * @returns {boolean} True if wake word detected
   */
  processFrame(frame) {
    if (!this.isInitialized || !this.porcupine) {
      return false;
    }

    try {
      const keywordIndex = this.porcupine.process(frame);
      if (keywordIndex >= 0) {
        console.log('[Porcupine] Wake word detected!');
        return true;
      }
      return false;
    } catch (error) {
      console.warn('[Porcupine] Frame processing error:', error.message);
      return false;
    }
  }

  /**
   * Release Porcupine resources
   */
  release() {
    if (this.porcupine) {
      this.porcupine.release();
      this.porcupine = null;
      this.isInitialized = false;
      console.log('[Porcupine] Released');
    }
  }
}

module.exports = { WakeWordDetector };
```

**Create `src/main/voice/whisperClient.js`:**

```javascript
/**
 * Whisper API Client for speech-to-text
 * Runs in Electron main process
 */

const OpenAI = require('openai');
const fs = require('fs');
const path = require('path');
const os = require('os');

class WhisperClient {
  constructor(apiKey) {
    this.openai = new OpenAI({ apiKey });
  }

  /**
   * Transcribe audio buffer to text
   * @param {Buffer} audioBuffer - Audio data (WebM/Opus format)
   * @returns {Promise<string>} Transcription text
   */
  async transcribe(audioBuffer) {
    if (!audioBuffer || audioBuffer.length === 0) {
      console.warn('[Whisper] Empty audio buffer received');
      return '';
    }

    // Write to temp file (Whisper API requires file upload)
    const tempPath = path.join(os.tmpdir(), `voice-${Date.now()}.webm`);

    try {
      fs.writeFileSync(tempPath, audioBuffer);
      console.log('[Whisper] Transcribing audio:', audioBuffer.length, 'bytes');

      const transcription = await this.openai.audio.transcriptions.create({
        file: fs.createReadStream(tempPath),
        model: 'whisper-1',
        language: 'en',
      });

      console.log('[Whisper] Transcription:', transcription.text);
      return transcription.text;
    } catch (error) {
      console.error('[Whisper] Transcription failed:', error.message);
      throw error;
    } finally {
      // Cleanup temp file
      try {
        if (fs.existsSync(tempPath)) {
          fs.unlinkSync(tempPath);
        }
      } catch (cleanupError) {
        console.warn('[Whisper] Temp file cleanup failed:', cleanupError.message);
      }
    }
  }
}

module.exports = { WhisperClient };
```
  </action>
  <verify>
1. File exists: `src/main/voice/wakeWordDetector.js`
2. File exists: `src/main/voice/whisperClient.js`
3. Run: `node -e "require('./src/main/voice/wakeWordDetector.js')"` - should not error
4. Run: `node -e "require('./src/main/voice/whisperClient.js')"` - should not error
  </verify>
  <done>WakeWordDetector and WhisperClient classes created</done>
</task>

<task type="auto">
  <name>Task 3: Create Voice IPC Handlers</name>
  <files>src/main/ipc/voiceHandlers.js</files>
  <action>
Create directory if needed: `mkdir -p src/main/ipc`

**Create `src/main/ipc/voiceHandlers.js`:**

```javascript
/**
 * Voice IPC Handlers
 * Bridge between renderer audio capture and main process voice processing
 */

const { ipcMain } = require('electron');
const path = require('path');
const fs = require('fs');
const { WakeWordDetector } = require('../voice/wakeWordDetector');
const { WhisperClient } = require('../voice/whisperClient');

let wakeWordDetector = null;
let whisperClient = null;
let mainWindow = null;

/**
 * Setup voice IPC handlers
 * @param {BrowserWindow} window - Main Electron window
 */
function setupVoiceIPC(window) {
  mainWindow = window;

  // Check voice availability
  ipcMain.handle('voice:check-availability', () => {
    const accessKey = process.env.PICOVOICE_ACCESS_KEY;
    const openaiKey = process.env.OPENAI_API_KEY;
    const modelPath = path.join(__dirname, '../../..', 'resources/wake-word/son-of-anton.ppn');
    const modelExists = fs.existsSync(modelPath);

    return {
      available: !!(accessKey && openaiKey && modelExists),
      hasAccessKey: !!accessKey,
      hasOpenAIKey: !!openaiKey,
      hasModel: modelExists,
    };
  });

  // Initialize voice services
  ipcMain.handle('voice:initialize', async () => {
    try {
      const accessKey = process.env.PICOVOICE_ACCESS_KEY;
      const openaiKey = process.env.OPENAI_API_KEY;

      if (!accessKey) {
        throw new Error('PICOVOICE_ACCESS_KEY not configured');
      }
      if (!openaiKey) {
        throw new Error('OPENAI_API_KEY not configured');
      }

      // Determine model path
      const modelPath = path.join(__dirname, '../../..', 'resources/wake-word/son-of-anton.ppn');
      if (!fs.existsSync(modelPath)) {
        throw new Error(`Wake word model not found at: ${modelPath}`);
      }

      wakeWordDetector = new WakeWordDetector(accessKey, modelPath);
      whisperClient = new WhisperClient(openaiKey);

      const audioReqs = wakeWordDetector.initialize();
      console.log('[VoiceIPC] Voice services initialized');

      return {
        success: true,
        ...audioReqs,
      };
    } catch (error) {
      console.error('[VoiceIPC] Initialization failed:', error.message);
      return {
        success: false,
        error: error.message,
      };
    }
  });

  // Process audio frame for wake word detection
  ipcMain.on('voice:audio-frame', (event, frameData) => {
    if (!wakeWordDetector) return;

    // Convert from regular array to Int16Array if needed
    const frame = frameData instanceof Int16Array
      ? frameData
      : new Int16Array(frameData);

    const detected = wakeWordDetector.processFrame(frame);
    if (detected) {
      event.sender.send('voice:wake-word-detected');
    }
  });

  // Transcribe audio with Whisper
  ipcMain.handle('voice:transcribe', async (event, audioData) => {
    if (!whisperClient) {
      return { success: false, error: 'Whisper client not initialized' };
    }

    try {
      // Convert from regular array to Buffer if needed
      const audioBuffer = Buffer.isBuffer(audioData)
        ? audioData
        : Buffer.from(audioData);

      const text = await whisperClient.transcribe(audioBuffer);
      return { success: true, text };
    } catch (error) {
      console.error('[VoiceIPC] Transcription failed:', error.message);
      return { success: false, error: error.message };
    }
  });

  // Release voice services
  ipcMain.on('voice:release', () => {
    if (wakeWordDetector) {
      wakeWordDetector.release();
      wakeWordDetector = null;
    }
    whisperClient = null;
    console.log('[VoiceIPC] Voice services released');
  });
}

/**
 * Cleanup voice IPC handlers
 */
function cleanupVoiceIPC() {
  if (wakeWordDetector) {
    wakeWordDetector.release();
    wakeWordDetector = null;
  }
  whisperClient = null;
}

module.exports = { setupVoiceIPC, cleanupVoiceIPC };
```
  </action>
  <verify>
1. File exists: `src/main/ipc/voiceHandlers.js`
2. Run: `node -e "require('./src/main/ipc/voiceHandlers.js')"` - should not error
3. Contains `ipcMain.handle` for voice:check-availability, voice:initialize, voice:transcribe
4. Contains `ipcMain.on` for voice:audio-frame, voice:release
  </verify>
  <done>Voice IPC handlers created for main-renderer communication</done>
</task>

</tasks>

<verification>
After all tasks complete:

1. **Dependencies installed:**
   ```bash
   npm ls @picovoice/porcupine-node
   ```

2. **Files created:**
   - `src/main/voice/wakeWordDetector.js`
   - `src/main/voice/whisperClient.js`
   - `src/main/ipc/voiceHandlers.js`
   - `resources/wake-word/son-of-anton.ppn` (user created)

3. **Module syntax valid:**
   ```bash
   node -e "require('./src/main/voice/wakeWordDetector.js')"
   node -e "require('./src/main/voice/whisperClient.js')"
   node -e "require('./src/main/ipc/voiceHandlers.js')"
   ```

4. **IPC handlers defined:**
   - voice:check-availability (handle)
   - voice:initialize (handle)
   - voice:audio-frame (on)
   - voice:transcribe (handle)
   - voice:release (on)
</verification>

<success_criteria>
1. Porcupine SDK installed and importable
2. WakeWordDetector processes audio frames and returns detection boolean
3. WhisperClient transcribes audio via OpenAI API
4. IPC handlers bridge renderer requests to main process modules
5. Modules standalone and testable without renderer integration
</success_criteria>

<output>
After completion, create `.planning/phases/09-voice-foundation/09-01-SUMMARY.md`
</output>
