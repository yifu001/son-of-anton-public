---
phase: 09-voice-foundation
plan: 03
type: execute
wave: 3
depends_on: ["09-02"]
files_modified:
  - src/_renderer.js
  - src/ui.html
  - src/assets/css/mod_voice.css
  - src/classes/audioFeedback.class.js
  - src/classes/waveformVisualizer.class.js
  - src/classes/voiceToggleWidget.class.js
  - src/classes/interimTranscription.class.js
files_created:
  - src/assets/css/mod_voice.css
  - src/classes/audioFeedback.class.js
  - src/classes/waveformVisualizer.class.js
  - src/classes/voiceToggleWidget.class.js
  - src/classes/interimTranscription.class.js
  - resources/sounds/yes-sir.mp3
  - resources/sounds/success.mp3
  - resources/sounds/failure.mp3
autonomous: false
user_setup:
  - service: audio_files
    why: "Voice feedback requires audio files"
    dashboard_config:
      - task: "Create or download 'Yes sir' voice response (TTS or recorded)"
        location: "Save to resources/sounds/yes-sir.mp3"
      - task: "Create or download success chime sound"
        location: "Save to resources/sounds/success.mp3"
      - task: "Create or download failure/error sound"
        location: "Save to resources/sounds/failure.mp3"

must_haves:
  truths:
    - "User sees voice toggle button on right side of screen"
    - "User hears 'Yes sir' when wake word is detected"
    - "User hears distinct sounds for success vs failure"
    - "User sees waveform animation at terminal bottom during recording"
    - "User sees partial transcription updating in real-time as they speak"
    - "User sees final transcription in terminal input line"
    - "User can click toggle to enable/disable voice listening"
  artifacts:
    - path: "src/classes/audioFeedback.class.js"
      provides: "Audio playback for Yes sir, success, failure sounds"
      contains: "playYesSir"
    - path: "src/classes/waveformVisualizer.class.js"
      provides: "Real-time audio waveform display"
      contains: "class WaveformVisualizer"
    - path: "src/classes/voiceToggleWidget.class.js"
      provides: "Voice enable/disable toggle button"
      contains: "class VoiceToggleWidget"
    - path: "src/classes/interimTranscription.class.js"
      provides: "Web Speech API for real-time interim transcription"
      contains: "webkitSpeechRecognition"
    - path: "src/assets/css/mod_voice.css"
      provides: "Styling for voice UI components"
      contains: ".voice-toggle"
  key_links:
    - from: "voiceController.class.js"
      to: "audioFeedback.class.js"
      via: "onWakeDetected callback"
      pattern: "playYesSir"
    - from: "voiceController.class.js"
      to: "waveformVisualizer.class.js"
      via: "onAudioLevel callback"
      pattern: "updateLevel"
    - from: "interimTranscription.class.js"
      to: "waveformVisualizer.class.js"
      via: "showInterim callback"
      pattern: "waveformVisualizer\\.showInterim"
    - from: "_renderer.js"
      to: "interimTranscription.class.js"
      via: "startInterim/stopInterim calls"
      pattern: "interimTranscription\\.start"
---

<objective>
Implement voice UX polish: "Yes sir" voice response, success/failure sounds, waveform visualization, voice toggle button, Web Speech API for interim transcription, and terminal input integration.

Purpose: Transform the core voice infrastructure (Plans 01-02) into a polished user experience with real-time feedback. Web Speech API runs in parallel with Whisper for local interim results (low latency) while Whisper provides accurate final transcription.

Output: Complete voice UX with toggle button, "Yes sir" confirmation, waveform with interim transcription, and transcription appearing directly in terminal input.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/09-voice-foundation/09-CONTEXT.md
@.planning/phases/09-voice-foundation/09-01-SUMMARY.md
@.planning/phases/09-voice-foundation/09-02-SUMMARY.md
</context>

<tasks>

<task type="checkpoint:human-action" gate="blocking">
  <name>Task 0: Create Audio Files for Voice Feedback</name>
  <files>resources/sounds/yes-sir.mp3, resources/sounds/success.mp3, resources/sounds/failure.mp3</files>
  <action>
**User must create or obtain these audio files:**

1. **"Yes sir" Voice Response** (required)
   - Create using TTS (text-to-speech) service OR record yourself
   - Suggested TTS services:
     - ElevenLabs (free tier): https://elevenlabs.io/
     - Google Cloud TTS: https://cloud.google.com/text-to-speech
     - macOS: `say "Yes sir" -o yes-sir.aiff && ffmpeg -i yes-sir.aiff yes-sir.mp3`
   - Duration: 0.5-1 second
   - Save to: `resources/sounds/yes-sir.mp3`

2. **Success Sound** (required)
   - Short positive chime/beep indicating transcription success
   - Free sources:
     - https://freesound.org/ (search "success chime")
     - https://mixkit.co/free-sound-effects/
   - Duration: 0.3-0.5 seconds
   - Save to: `resources/sounds/success.mp3`

3. **Failure Sound** (required)
   - Short negative sound indicating transcription failure
   - Free sources: Same as above (search "error beep")
   - Duration: 0.3-0.5 seconds
   - Save to: `resources/sounds/failure.mp3`

**Directory creation:**
```bash
mkdir -p resources/sounds
```
  </action>
  <verify>
1. File exists: `resources/sounds/yes-sir.mp3`
2. File exists: `resources/sounds/success.mp3`
3. File exists: `resources/sounds/failure.mp3`
4. All files are playable MP3 audio
  </verify>
  <done>Audio files created/downloaded for voice feedback</done>
  <resume-signal>Type "done" when audio files are ready</resume-signal>
</task>

<task type="auto">
  <name>Task 1: Create Audio Feedback, Waveform, and Toggle Classes</name>
  <files>src/classes/audioFeedback.class.js, src/classes/waveformVisualizer.class.js, src/classes/voiceToggleWidget.class.js</files>
  <action>
**Create `src/classes/audioFeedback.class.js`:**

```javascript
/**
 * Audio Feedback Service
 * Handles voice responses and audio cues for voice interaction
 *
 * Context decisions implemented:
 * - "Yes sir" voice response on wake word detection (not generic chime)
 * - Distinct success sound on transcription success
 * - Distinct failure sound on transcription failure
 */

const path = require('path');
const { Howl, Howler } = require('howler');

class AudioFeedback {
  constructor() {
    this.yesSir = null;
    this.success = null;
    this.failure = null;
    this.isLoaded = false;
  }

  /**
   * Initialize audio feedback with preloaded sounds
   * @returns {boolean} True if initialization successful
   */
  initialize() {
    try {
      const soundsDir = path.join(__dirname, '..', 'resources', 'sounds');

      // "Yes sir" voice response for wake word detection
      this.yesSir = new Howl({
        src: [path.join(soundsDir, 'yes-sir.mp3')],
        volume: 0.7,
        preload: true,
        onload: () => console.log('[AudioFeedback] Yes sir loaded'),
        onloaderror: (id, err) => console.warn('[AudioFeedback] Yes sir load error:', err),
      });

      // Success sound for transcription completion
      this.success = new Howl({
        src: [path.join(soundsDir, 'success.mp3')],
        volume: 0.5,
        preload: true,
        onload: () => console.log('[AudioFeedback] Success sound loaded'),
        onloaderror: (id, err) => console.warn('[AudioFeedback] Success load error:', err),
      });

      // Failure sound for transcription errors
      this.failure = new Howl({
        src: [path.join(soundsDir, 'failure.mp3')],
        volume: 0.5,
        preload: true,
        onload: () => console.log('[AudioFeedback] Failure sound loaded'),
        onloaderror: (id, err) => console.warn('[AudioFeedback] Failure load error:', err),
      });

      // Respect global audio settings
      if (window.settings && window.settings.audio === false) {
        Howler.volume(0);
      } else if (window.settings && typeof window.settings.audioVolume === 'number') {
        Howler.volume(window.settings.audioVolume);
      }

      this.isLoaded = true;
      console.log('[AudioFeedback] Initialized');
      return true;
    } catch (error) {
      console.error('[AudioFeedback] Initialization failed:', error.message);
      return false;
    }
  }

  playYesSir() {
    if (this.yesSir) {
      this.yesSir.play();
      console.log('[AudioFeedback] Playing: Yes sir');
    }
  }

  playSuccess() {
    if (this.success) {
      this.success.play();
      console.log('[AudioFeedback] Playing: Success');
    }
  }

  playFailure() {
    if (this.failure) {
      this.failure.play();
      console.log('[AudioFeedback] Playing: Failure');
    }
  }

  setVolume(volume) {
    Howler.volume(Math.max(0, Math.min(1, volume)));
  }

  release() {
    if (this.yesSir) this.yesSir.unload();
    if (this.success) this.success.unload();
    if (this.failure) this.failure.unload();
    this.yesSir = null;
    this.success = null;
    this.failure = null;
    this.isLoaded = false;
    console.log('[AudioFeedback] Released');
  }
}

module.exports = { AudioFeedback };
```

**Create `src/classes/waveformVisualizer.class.js`:**

```javascript
/**
 * Waveform Visualizer
 * Real-time audio waveform display at bottom of active terminal
 *
 * Context decisions implemented:
 * - Waveform visualization showing audio input levels in real-time
 * - Position: Bottom of active terminal (not floating)
 * - Shows interim transcription text
 */

class WaveformVisualizer {
  constructor(options = {}) {
    this.barCount = options.barCount || 32;
    this.container = null;
    this.bars = [];
    this.isVisible = false;
    this.targetTerminalIndex = null;
    this.animationFrame = null;
    this.levels = new Array(this.barCount).fill(0);
    this.levelIndex = 0;
  }

  _createDOM() {
    this.container = document.createElement('div');
    this.container.className = 'voice-waveform';
    this.container.innerHTML = `
      <div class="voice-waveform__label">VOICE INPUT</div>
      <div class="voice-waveform__bars"></div>
      <div class="voice-waveform__status">Recording...</div>
    `;

    const barsContainer = this.container.querySelector('.voice-waveform__bars');
    for (let i = 0; i < this.barCount; i++) {
      const bar = document.createElement('div');
      bar.className = 'voice-waveform__bar';
      barsContainer.appendChild(bar);
      this.bars.push(bar);
    }

    return this.container;
  }

  show(terminalIndex) {
    if (this.isVisible) {
      this.hide();
    }

    this.targetTerminalIndex = terminalIndex;

    const terminalSelector = `#mod_term_${terminalIndex}`;
    const terminalEl = document.querySelector(terminalSelector);

    if (!terminalEl) {
      const termWrapper = document.querySelector('.mod_term_wrapper') ||
                          document.querySelector('#mod_terminals');
      if (termWrapper) {
        if (!this.container) this._createDOM();
        termWrapper.appendChild(this.container);
      } else {
        console.warn('[WaveformVisualizer] Cannot find terminal container');
        return;
      }
    } else {
      if (!this.container) this._createDOM();
      terminalEl.appendChild(this.container);
    }

    this.container.classList.add('voice-waveform--visible');
    this.isVisible = true;
    this._startAnimation();

    console.log('[WaveformVisualizer] Shown at terminal', terminalIndex);
  }

  hide() {
    if (!this.isVisible) return;

    this._stopAnimation();

    if (this.container) {
      this.container.classList.remove('voice-waveform--visible');
      setTimeout(() => {
        if (this.container && this.container.parentNode) {
          this.container.parentNode.removeChild(this.container);
        }
      }, 300);
    }

    this.isVisible = false;
    this.targetTerminalIndex = null;
    console.log('[WaveformVisualizer] Hidden');
  }

  updateLevel(level) {
    this.levels[this.levelIndex] = level;
    this.levelIndex = (this.levelIndex + 1) % this.barCount;
  }

  setStatus(status) {
    if (this.container) {
      const statusEl = this.container.querySelector('.voice-waveform__status');
      if (statusEl) {
        statusEl.textContent = status;
        statusEl.classList.remove('voice-waveform__status--interim');
      }
    }
  }

  /**
   * Show interim transcription text - KEY LINK for Web Speech API
   * @param {string} text - Interim transcription from Web Speech API
   */
  showInterim(text) {
    if (this.container) {
      const statusEl = this.container.querySelector('.voice-waveform__status');
      if (statusEl && text) {
        statusEl.textContent = text;
        statusEl.classList.add('voice-waveform__status--interim');
      }
    }
  }

  _startAnimation() {
    const animate = () => {
      this._renderBars();
      this.animationFrame = requestAnimationFrame(animate);
    };
    animate();
  }

  _stopAnimation() {
    if (this.animationFrame) {
      cancelAnimationFrame(this.animationFrame);
      this.animationFrame = null;
    }
  }

  _renderBars() {
    for (let i = 0; i < this.barCount; i++) {
      const levelIdx = (this.levelIndex + i) % this.barCount;
      const level = this.levels[levelIdx];
      const jitter = Math.random() * 0.1;
      const height = Math.max(0.05, Math.min(1, level + jitter));

      this.bars[i].style.height = `${height * 100}%`;

      const intensity = Math.floor(200 + level * 55);
      this.bars[i].style.backgroundColor = `rgb(${intensity}, ${intensity}, ${intensity})`;
    }
  }

  release() {
    this.hide();
    this.container = null;
    this.bars = [];
    console.log('[WaveformVisualizer] Released');
  }
}

module.exports = { WaveformVisualizer };
```

**Create `src/classes/voiceToggleWidget.class.js`:**

```javascript
/**
 * Voice Toggle Widget
 * Button on right side of screen to enable/disable voice listening
 *
 * Context decisions implemented:
 * - UI button on RIGHT SIDE of screen (not settings)
 * - Toggle enables/disables voice listening
 */

class VoiceToggleWidget {
  constructor(voiceController) {
    this.voiceController = voiceController;
    this.element = null;
    this.isEnabled = false;
  }

  create(container) {
    this.element = document.createElement('div');
    this.element.id = 'mod_voiceToggle';
    this.element.className = 'voice-toggle';
    this.element.innerHTML = `
      <div class="voice-toggle__inner">
        <div class="voice-toggle__icon">
          <svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
            <path d="M12 1a3 3 0 0 0-3 3v8a3 3 0 0 0 6 0V4a3 3 0 0 0-3-3z"/>
            <path d="M19 10v2a7 7 0 0 1-14 0v-2"/>
            <line x1="12" y1="19" x2="12" y2="23"/>
            <line x1="8" y1="23" x2="16" y2="23"/>
          </svg>
        </div>
        <div class="voice-toggle__label">VOICE</div>
        <div class="voice-toggle__status">OFF</div>
      </div>
    `;

    this.element.addEventListener('click', () => this._handleClick());

    if (container) {
      container.appendChild(this.element);
    }

    this._updateVisualState();
    console.log('[VoiceToggle] Widget created');
    return this.element;
  }

  _handleClick() {
    if (!this.voiceController) {
      console.warn('[VoiceToggle] No voice controller');
      return;
    }

    const newState = this.voiceController.toggle();
    this.isEnabled = newState;
    this._updateVisualState();

    console.log('[VoiceToggle] Toggled to:', newState ? 'ON' : 'OFF');
  }

  _updateVisualState() {
    if (!this.element) return;

    const statusEl = this.element.querySelector('.voice-toggle__status');

    if (this.isEnabled) {
      this.element.classList.add('voice-toggle--active');
      this.element.classList.remove('voice-toggle--inactive');
      if (statusEl) statusEl.textContent = 'ON';
    } else {
      this.element.classList.remove('voice-toggle--active');
      this.element.classList.add('voice-toggle--inactive');
      if (statusEl) statusEl.textContent = 'OFF';
    }
  }

  setEnabled(enabled) {
    this.isEnabled = enabled;
    this._updateVisualState();
  }

  showRecording() {
    if (this.element) {
      this.element.classList.add('voice-toggle--recording');
      const statusEl = this.element.querySelector('.voice-toggle__status');
      if (statusEl) statusEl.textContent = 'REC';
    }
  }

  showProcessing() {
    if (this.element) {
      this.element.classList.remove('voice-toggle--recording');
      this.element.classList.add('voice-toggle--processing');
      const statusEl = this.element.querySelector('.voice-toggle__status');
      if (statusEl) statusEl.textContent = '...';
    }
  }

  resetState() {
    if (this.element) {
      this.element.classList.remove('voice-toggle--recording', 'voice-toggle--processing');
      this._updateVisualState();
    }
  }

  showUnavailable() {
    if (this.element) {
      this.element.classList.add('voice-toggle--unavailable');
      const statusEl = this.element.querySelector('.voice-toggle__status');
      if (statusEl) statusEl.textContent = 'N/A';
    }
  }

  release() {
    if (this.element && this.element.parentNode) {
      this.element.parentNode.removeChild(this.element);
    }
    this.element = null;
    console.log('[VoiceToggle] Released');
  }
}

module.exports = { VoiceToggleWidget };
```
  </action>
  <verify>
1. File exists: `src/classes/audioFeedback.class.js`
2. File exists: `src/classes/waveformVisualizer.class.js`
3. File exists: `src/classes/voiceToggleWidget.class.js`
4. WaveformVisualizer contains `showInterim(text)` method
  </verify>
  <done>Audio feedback, waveform visualizer, and toggle widget classes created</done>
</task>

<task type="auto">
  <name>Task 2: Create Interim Transcription Class with Web Speech API</name>
  <files>src/classes/interimTranscription.class.js</files>
  <action>
**Create `src/classes/interimTranscription.class.js`:**

This class uses Web Speech API for local, low-latency interim transcription that runs in parallel with Whisper API. Web Speech provides real-time feedback while Whisper provides accurate final results.

```javascript
/**
 * Interim Transcription Service
 * Uses Web Speech API for real-time interim transcription display
 *
 * Context decisions implemented:
 * - Interim results: Show partial transcription in real-time as user speaks
 * - Runs in parallel with Whisper API (Whisper for final, Web Speech for interim)
 *
 * Why Web Speech API:
 * - Local processing = low latency (~200ms)
 * - Provides interim results as user speaks
 * - No API cost for interim display
 * - Whisper API still provides final accurate transcription
 */

class InterimTranscription {
  constructor(options = {}) {
    this.recognition = null;
    this.isSupported = false;
    this.isRunning = false;

    // Callbacks
    this.onInterim = options.onInterim || (() => {});
    this.onError = options.onError || (() => {});

    // Check browser support
    this._checkSupport();
  }

  /**
   * Check if Web Speech API is supported
   * @private
   */
  _checkSupport() {
    // webkitSpeechRecognition is available in Chromium-based browsers (Electron uses Chromium)
    const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;

    if (SpeechRecognition) {
      this.isSupported = true;
      this.recognition = new SpeechRecognition();
      this._configure();
      console.log('[InterimTranscription] Web Speech API supported');
    } else {
      this.isSupported = false;
      console.warn('[InterimTranscription] Web Speech API not supported');
    }
  }

  /**
   * Configure speech recognition
   * @private
   */
  _configure() {
    if (!this.recognition) return;

    // Configuration for interim results
    this.recognition.continuous = true;        // Keep listening until stopped
    this.recognition.interimResults = true;    // Get interim (partial) results
    this.recognition.lang = 'en-US';           // English language
    this.recognition.maxAlternatives = 1;      // Just need one result

    // Handle results
    this.recognition.onresult = (event) => {
      let interimTranscript = '';

      // Collect interim results
      for (let i = event.resultIndex; i < event.results.length; i++) {
        const result = event.results[i];
        if (!result.isFinal) {
          interimTranscript += result[0].transcript;
        }
      }

      // Send interim text to callback
      if (interimTranscript) {
        this.onInterim(interimTranscript);
      }
    };

    // Handle errors silently (Web Speech is supplementary)
    this.recognition.onerror = (event) => {
      // Common errors that don't need user notification:
      // - 'aborted': We stopped it
      // - 'no-speech': User was silent
      // - 'network': Offline mode
      if (event.error !== 'aborted' && event.error !== 'no-speech') {
        console.warn('[InterimTranscription] Error:', event.error);
        this.onError(event.error);
      }
    };

    // Auto-restart if it stops unexpectedly while we want it running
    this.recognition.onend = () => {
      if (this.isRunning) {
        // Restart recognition
        try {
          this.recognition.start();
        } catch (e) {
          // Already running or other error, ignore
        }
      }
    };
  }

  /**
   * Start interim transcription
   * Call this when recording starts (after wake word)
   */
  start() {
    if (!this.isSupported || !this.recognition) {
      console.warn('[InterimTranscription] Cannot start - not supported');
      return false;
    }

    if (this.isRunning) {
      console.warn('[InterimTranscription] Already running');
      return true;
    }

    try {
      this.isRunning = true;
      this.recognition.start();
      console.log('[InterimTranscription] Started');
      return true;
    } catch (error) {
      console.error('[InterimTranscription] Start failed:', error.message);
      this.isRunning = false;
      return false;
    }
  }

  /**
   * Stop interim transcription
   * Call this when recording stops
   */
  stop() {
    if (!this.recognition) return;

    this.isRunning = false;

    try {
      this.recognition.stop();
      console.log('[InterimTranscription] Stopped');
    } catch (error) {
      // Ignore stop errors (might not be running)
    }
  }

  /**
   * Check if running
   */
  getIsRunning() {
    return this.isRunning;
  }

  /**
   * Check if supported
   */
  getIsSupported() {
    return this.isSupported;
  }

  /**
   * Release resources
   */
  release() {
    this.stop();
    this.recognition = null;
    this.isSupported = false;
    console.log('[InterimTranscription] Released');
  }
}

module.exports = { InterimTranscription };
```
  </action>
  <verify>
1. File exists: `src/classes/interimTranscription.class.js`
2. Contains `webkitSpeechRecognition`
3. Contains `interimResults = true`
4. Contains `onInterim` callback
  </verify>
  <done>Interim transcription class created with Web Speech API</done>
</task>

<task type="auto">
  <name>Task 3: Create Voice CSS and Integrate into Renderer</name>
  <files>src/assets/css/mod_voice.css, src/ui.html, src/_renderer.js</files>
  <action>
**Create `src/assets/css/mod_voice.css`:**

```css
/**
 * Voice Module Styles
 * Styling for voice toggle button and waveform visualizer
 */

/* Voice Toggle Widget (Right Side) */
.voice-toggle {
  border-top: 0.092vh solid rgba(var(--color_r), var(--color_g), var(--color_b), 0.3);
  font-family: var(--font_main_light);
  letter-spacing: 0.092vh;
  padding: 0.8vh 0;
  cursor: pointer;
  transition: all 0.2s ease;
  user-select: none;
}

.voice-toggle::before {
  content: "";
  border-left: 0.092vh solid rgba(var(--color_r), var(--color_g), var(--color_b), 0.3);
  align-self: flex-start;
  position: relative;
  left: -0.092vh;
  top: -1.111vh;
  height: 0.833vh;
}

.voice-toggle::after {
  content: "";
  border-right: 0.092vh solid rgba(var(--color_r), var(--color_g), var(--color_b), 0.3);
  position: relative;
  right: -0.092vh;
  top: -1.111vh;
  height: 0.833vh;
}

.voice-toggle__inner {
  display: flex;
  flex-direction: column;
  align-items: center;
  padding: 0.5vh;
}

.voice-toggle__icon {
  width: 2.5vh;
  height: 2.5vh;
  color: rgba(var(--color_r), var(--color_g), var(--color_b), 0.5);
  transition: color 0.2s ease;
}

.voice-toggle__icon svg {
  width: 100%;
  height: 100%;
}

.voice-toggle__label {
  font-size: 0.9vh;
  color: rgba(var(--color_r), var(--color_g), var(--color_b), 0.7);
  margin-top: 0.3vh;
}

.voice-toggle__status {
  font-size: 1.2vh;
  font-weight: bold;
  color: rgba(var(--color_r), var(--color_g), var(--color_b), 0.5);
  margin-top: 0.2vh;
}

.voice-toggle--active .voice-toggle__icon {
  color: rgb(var(--color_r), var(--color_g), var(--color_b));
}

.voice-toggle--active .voice-toggle__status {
  color: #22c55e;
}

.voice-toggle--inactive {
  opacity: 0.6;
}

.voice-toggle--recording .voice-toggle__icon {
  color: #ef4444;
  animation: voice-pulse 1s ease-in-out infinite;
}

.voice-toggle--recording .voice-toggle__status {
  color: #ef4444;
}

.voice-toggle--processing .voice-toggle__icon {
  color: #eab308;
}

.voice-toggle--processing .voice-toggle__status {
  color: #eab308;
}

.voice-toggle--unavailable {
  opacity: 0.3;
  cursor: not-allowed;
}

.voice-toggle:hover:not(.voice-toggle--unavailable) {
  background: rgba(var(--color_r), var(--color_g), var(--color_b), 0.05);
}

@keyframes voice-pulse {
  0%, 100% { opacity: 1; transform: scale(1); }
  50% { opacity: 0.7; transform: scale(1.1); }
}

/* Waveform Visualizer (Terminal Bottom) */
.voice-waveform {
  position: absolute;
  bottom: 0;
  left: 0;
  right: 0;
  height: 6vh;
  background: linear-gradient(to top, rgba(0, 0, 0, 0.9) 0%, rgba(0, 0, 0, 0.7) 50%, transparent 100%);
  border-top: 1px solid rgba(var(--color_r), var(--color_g), var(--color_b), 0.3);
  display: flex;
  flex-direction: column;
  align-items: center;
  justify-content: center;
  z-index: 100;
  opacity: 0;
  transform: translateY(100%);
  transition: opacity 0.3s ease, transform 0.3s ease;
  pointer-events: none;
}

.voice-waveform--visible {
  opacity: 1;
  transform: translateY(0);
}

.voice-waveform__label {
  font-family: var(--font_main);
  font-size: 0.8vh;
  color: rgba(var(--color_r), var(--color_g), var(--color_b), 0.6);
  letter-spacing: 0.2vh;
  margin-bottom: 0.5vh;
}

.voice-waveform__bars {
  display: flex;
  align-items: flex-end;
  justify-content: center;
  height: 3vh;
  gap: 2px;
}

.voice-waveform__bar {
  width: 4px;
  min-height: 2px;
  background: rgb(var(--color_r), var(--color_g), var(--color_b));
  border-radius: 1px;
  transition: height 0.05s ease-out;
}

.voice-waveform__status {
  font-family: var(--font_mono);
  font-size: 1vh;
  color: rgba(var(--color_r), var(--color_g), var(--color_b), 0.8);
  margin-top: 0.5vh;
  max-width: 80%;
  white-space: nowrap;
  overflow: hidden;
  text-overflow: ellipsis;
}

.voice-waveform__status--interim {
  color: rgba(var(--color_r), var(--color_g), var(--color_b), 0.6);
  font-style: italic;
}

/* Terminal Input Voice Integration */
.term-input--voice-active {
  border-color: rgba(var(--color_r), var(--color_g), var(--color_b), 0.8) !important;
  box-shadow: 0 0 5px rgba(var(--color_r), var(--color_g), var(--color_b), 0.3);
}
```

**Add to `src/ui.html` in the <head> section after other module CSS:**

```html
<link rel="stylesheet" href="assets/css/mod_voice.css">
```

**Add to `src/_renderer.js` (see detailed action below):**

1. Add requires after other class imports:
```javascript
const { VoiceController, VoiceState } = require('./classes/voiceController.class');
const { AudioFeedback } = require('./classes/audioFeedback.class');
const { WaveformVisualizer } = require('./classes/waveformVisualizer.class');
const { VoiceToggleWidget } = require('./classes/voiceToggleWidget.class');
const { InterimTranscription } = require('./classes/interimTranscription.class');
```

2. Add voice globals after window.claudeState:
```javascript
// Voice control instances
window.voiceController = null;
window.audioFeedback = null;
window.waveformVisualizer = null;
window.voiceToggleWidget = null;
window.interimTranscription = null;

// IPC wrapper for voice
window.ipc = {
  invoke: (channel, ...args) => ipc.invoke(channel, ...args),
  send: (channel, ...args) => ipc.send(channel, ...args),
  on: (channel, callback) => ipc.on(channel, (event, ...args) => callback(...args)),
};
```

3. Add initializeVoice function:
```javascript
async function initializeVoice() {
  console.log('[Voice] Initializing voice system...');

  // Create audio feedback handler
  window.audioFeedback = new AudioFeedback();
  window.audioFeedback.initialize();

  // Create waveform visualizer
  window.waveformVisualizer = new WaveformVisualizer({ barCount: 32 });

  // Create interim transcription (Web Speech API)
  window.interimTranscription = new InterimTranscription({
    onInterim: (text) => {
      // Wire interim results to waveform visualizer
      if (window.waveformVisualizer) {
        window.waveformVisualizer.showInterim(text);
      }
    },
    onError: (error) => {
      console.warn('[Voice] Interim transcription error:', error);
    },
  });

  // Create voice controller with callbacks
  window.voiceController = new VoiceController({
    maxRecordingMs: 60000,
    silenceTimeoutMs: window.settings.voiceSilenceTimeout || 2000,

    onStateChange: (state, oldState) => {
      console.log('[Voice] State changed:', oldState, '->', state);

      // Update toggle widget
      if (window.voiceToggleWidget) {
        if (state === VoiceState.RECORDING) {
          window.voiceToggleWidget.showRecording();
        } else if (state === VoiceState.PROCESSING) {
          window.voiceToggleWidget.showProcessing();
        } else {
          window.voiceToggleWidget.resetState();
        }
      }

      // Show/hide waveform and manage interim transcription
      if (state === VoiceState.RECORDING) {
        const activeTerminal = window.activeTerminal || 0;
        window.waveformVisualizer.show(activeTerminal);
        // Start Web Speech API for interim results
        if (window.interimTranscription) {
          window.interimTranscription.start();
        }
      } else if (oldState === VoiceState.RECORDING) {
        window.waveformVisualizer.hide();
        // Stop Web Speech API
        if (window.interimTranscription) {
          window.interimTranscription.stop();
        }
      }
    },

    onWakeDetected: () => {
      window.audioFeedback.playYesSir();
    },

    onTranscription: (text, success) => {
      if (success && text) {
        window.audioFeedback.playSuccess();
        insertTranscriptionIntoTerminal(text);
      } else {
        window.audioFeedback.playFailure();
      }
    },

    onAudioLevel: (level) => {
      if (window.waveformVisualizer) {
        window.waveformVisualizer.updateLevel(level);
      }
    },

    onError: (error) => {
      console.error('[Voice] Error:', error);
    },
  });

  const initialized = await window.voiceController.initialize();

  // Create toggle widget in right column
  const rightColumn = document.querySelector('#mod_column_right');
  if (rightColumn) {
    window.voiceToggleWidget = new VoiceToggleWidget(window.voiceController);
    window.voiceToggleWidget.create(rightColumn);

    if (!initialized) {
      window.voiceToggleWidget.showUnavailable();
    }
  }

  console.log('[Voice] Voice system initialized:', initialized ? 'SUCCESS' : 'UNAVAILABLE');
}

function insertTranscriptionIntoTerminal(text) {
  const activeTerminal = window.activeTerminal || 0;

  const terminalInput = document.querySelector(`#term_${activeTerminal} input`) ||
                        document.querySelector(`.term-input[data-term="${activeTerminal}"]`);

  if (terminalInput) {
    terminalInput.value = text;
    terminalInput.classList.add('term-input--voice-active');
    terminalInput.focus();

    setTimeout(() => {
      terminalInput.classList.remove('term-input--voice-active');
    }, 2000);

    console.log('[Voice] Inserted transcription into terminal', activeTerminal);
    return;
  }

  if (window.term && window.term[activeTerminal]) {
    const term = window.term[activeTerminal];
    term.write(text);
    console.log('[Voice] Wrote transcription to xterm', activeTerminal);
    return;
  }

  console.warn('[Voice] Could not find terminal input for index', activeTerminal);
}
```

4. Call initializeVoice after DOM ready (find where other modules are initialized and add):
```javascript
initializeVoice();
```
  </action>
  <verify>
1. File exists: `src/assets/css/mod_voice.css`
2. `grep -n "mod_voice.css" src/ui.html` - shows link tag
3. `grep -n "VoiceController" src/_renderer.js` - shows import and instantiation
4. `grep -n "interimTranscription" src/_renderer.js` - shows Web Speech API integration
5. `grep -n "showInterim" src/_renderer.js` - shows wiring to waveform
  </verify>
  <done>Voice CSS and renderer integration with Web Speech API interim transcription complete</done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <name>Task 4: Verify Voice UX End-to-End</name>
  <what-built>
Complete voice UX integration:
- Voice toggle button on right side of screen
- "Yes sir" voice response on wake word detection
- Success/failure sounds on transcription result
- Waveform visualization at terminal bottom during recording
- **Interim transcription via Web Speech API showing as user speaks**
- Final transcription (Whisper) appearing in terminal input
- 60 second max duration
- Space key cancel
  </what-built>
  <how-to-verify>
1. **Start the app:**
   ```bash
   npm start
   ```

2. **Check toggle button:**
   - Look for VOICE toggle button in right column
   - Click to enable (should show ON, icon brightens)

3. **Test wake word:**
   - Say "Son of Anton" clearly
   - Expected: "Yes sir" voice response plays
   - Expected: Waveform visualization appears at terminal bottom
   - Expected: Toggle shows REC state

4. **Test interim transcription (KEY NEW FEATURE):**
   - While recording, speak continuously
   - Expected: Waveform status area shows text updating in real-time
   - Expected: Text appears in italic style (interim indicator)
   - This is Web Speech API providing local, low-latency feedback

5. **Test final transcription:**
   - Wait 2 seconds for silence timeout
   - Expected: Success chime plays
   - Expected: Final text (from Whisper) appears in terminal input area
   - Note: Final text may differ slightly from interim (Whisper is more accurate)

6. **Test space cancel:**
   - Say "Son of Anton" to start recording
   - Press Space key
   - Expected: Recording cancelled, returns to listening

7. **Test failure (optional):**
   - Remove OPENAI_API_KEY temporarily
   - Try voice input
   - Expected: Failure sound plays
  </how-to-verify>
  <resume-signal>Type "approved" if all tests pass, or describe issues found</resume-signal>
</task>

</tasks>

<verification>
After all tasks complete:

1. **Audio files exist:**
   ```bash
   ls resources/sounds/
   # Should show: yes-sir.mp3, success.mp3, failure.mp3
   ```

2. **Classes created:**
   ```bash
   ls src/classes/audio*.class.js src/classes/waveform*.class.js src/classes/voice*.class.js src/classes/interim*.class.js
   ```

3. **CSS created:**
   ```bash
   cat src/assets/css/mod_voice.css | head -20
   ```

4. **Web Speech API wiring:**
   ```bash
   grep -n "showInterim" src/_renderer.js
   grep -n "interimTranscription.start" src/_renderer.js
   ```

5. **Context decisions verified:**
   - [ ] Voice toggle on RIGHT SIDE: VoiceToggleWidget in #mod_column_right
   - [ ] "Yes sir" response: AudioFeedback.playYesSir() in onWakeDetected
   - [ ] Success/failure sounds: playSuccess/playFailure in onTranscription
   - [ ] Waveform at terminal bottom: WaveformVisualizer.show(terminalIndex)
   - [ ] **Interim transcription: InterimTranscription -> waveformVisualizer.showInterim()**
   - [ ] Transcription in terminal input: insertTranscriptionIntoTerminal()
</verification>

<success_criteria>
1. Voice toggle button visible and functional on right side
2. "Yes sir" voice plays on wake word detection
3. Success sound plays on successful transcription
4. Failure sound plays on transcription error
5. Waveform visualization appears at bottom of active terminal
6. **Interim transcription displays in real-time via Web Speech API**
7. Final transcription (Whisper) appears in terminal input line
8. Space key immediately cancels recording
</success_criteria>

<output>
After completion, create `.planning/phases/09-voice-foundation/09-03-SUMMARY.md`
</output>
