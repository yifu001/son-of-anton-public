---
phase: 09-voice-foundation
plan: 02
type: execute
wave: 2
depends_on: ["09-01"]
files_modified:
  - src/_boot.js
  - src/classes/audioCapture.class.js
  - src/classes/voiceController.class.js
files_created:
  - src/classes/audioCapture.class.js
  - src/classes/voiceController.class.js
autonomous: true

must_haves:
  truths:
    - "App requests microphone permission when voice is enabled"
    - "Audio frames stream continuously from microphone to wake word detector"
    - "Voice state changes from IDLE to LISTENING to RECORDING to PROCESSING"
    - "Recording stops after 60 seconds maximum"
    - "Space key cancels recording immediately"
  artifacts:
    - path: "src/classes/audioCapture.class.js"
      provides: "Microphone capture with frame extraction"
      contains: "navigator.mediaDevices.getUserMedia"
    - path: "src/classes/voiceController.class.js"
      provides: "Voice state machine orchestration"
      contains: "class VoiceController"
    - path: "src/_boot.js"
      provides: "Voice IPC initialization on app startup"
      contains: "setupVoiceIPC"
  key_links:
    - from: "audioCapture.class.js"
      to: "voiceHandlers.js"
      via: "IPC voice:audio-frame"
      pattern: "send.*voice:audio-frame"
    - from: "voiceController.class.js"
      to: "audioCapture.class.js"
      via: "startFrameCapture callback"
      pattern: "audioCapture.startFrameCapture"
    - from: "_boot.js"
      to: "voiceHandlers.js"
      via: "setupVoiceIPC import and call"
      pattern: "setupVoiceIPC\\(win\\)"
---

<objective>
Wire voice infrastructure into the application: boot integration, audio capture from microphone, and voice state machine.

Purpose: Connect the main-process modules from Plan 01 to the renderer, creating a working voice pipeline that captures audio and manages state transitions.

Output: Working voice capture and state machine, ready for UX polish in Plan 03.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/09-voice-foundation/09-CONTEXT.md
@.planning/phases/09-voice-foundation/09-01-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Integrate Voice IPC into Boot Process</name>
  <files>src/_boot.js</files>
  <action>
Modify `src/_boot.js` to initialize voice IPC handlers.

**1. Add require at top of file (after other requires around line 40):**
```javascript
const { setupVoiceIPC, cleanupVoiceIPC } = require('./main/ipc/voiceHandlers');
```

**2. Add voice IPC setup after claudeStateManager initialization (around line 227):**
Find this block:
```javascript
win.webContents.on('did-finish-load', () => {
    claudeStateManager = new ClaudeStateManager(win);
    claudeStateManager.start();
    signale.success("Claude state manager initialized");
});
```

Add after `claudeStateManager.start()`:
```javascript
    // Initialize voice IPC handlers
    setupVoiceIPC(win);
    signale.success("Voice IPC handlers initialized");
```

**3. Add cleanup on app quit (add before or after the uncaughtException handler at top):**
```javascript
app.on('will-quit', () => {
    cleanupVoiceIPC();
});
```
  </action>
  <verify>
1. `grep -n "setupVoiceIPC" src/_boot.js` - should show import and call
2. `grep -n "cleanupVoiceIPC" src/_boot.js` - should show cleanup
3. `grep -n "will-quit" src/_boot.js` - should show cleanup handler
  </verify>
  <done>Voice IPC integrated into Electron boot process</done>
</task>

<task type="auto">
  <name>Task 2: Create Audio Capture Class</name>
  <files>src/classes/audioCapture.class.js</files>
  <action>
Create `src/classes/audioCapture.class.js`:

```javascript
/**
 * Audio Capture Service
 * Handles microphone access and audio frame extraction for wake word detection
 */

class AudioCapture {
  constructor() {
    this.audioContext = null;
    this.mediaStream = null;
    this.processor = null;
    this.mediaRecorder = null;
    this.audioChunks = [];
    this.isCapturing = false;
    this.onAudioFrame = null;
    this.analyser = null;
    this.dataArray = null;
  }

  /**
   * Request microphone permission
   * @returns {Promise<boolean>} True if permission granted
   */
  async requestPermission() {
    try {
      this.mediaStream = await navigator.mediaDevices.getUserMedia({
        audio: {
          sampleRate: 16000,
          channelCount: 1,
          echoCancellation: true,
          noiseSuppression: true,
        },
      });
      console.log('[AudioCapture] Microphone permission granted');
      return true;
    } catch (error) {
      console.error('[AudioCapture] Microphone permission denied:', error.message);
      return false;
    }
  }

  /**
   * Check if microphone is available
   */
  hasPermission() {
    return this.mediaStream !== null;
  }

  /**
   * Start capturing audio frames for wake word detection
   * @param {Function} onFrame - Callback receiving Int16Array frames
   */
  startFrameCapture(onFrame) {
    if (!this.mediaStream) {
      console.error('[AudioCapture] No media stream available');
      return false;
    }

    this.onAudioFrame = onFrame;
    this.audioContext = new AudioContext({ sampleRate: 16000 });
    const source = this.audioContext.createMediaStreamSource(this.mediaStream);

    // Process in 512-sample frames for Porcupine
    this.processor = this.audioContext.createScriptProcessor(512, 1, 1);
    this.processor.onaudioprocess = (event) => {
      if (!this.onAudioFrame) return;

      const inputData = event.inputBuffer.getChannelData(0);
      const frame = this._float32ToInt16(inputData);
      this.onAudioFrame(frame);
    };

    source.connect(this.processor);
    this.processor.connect(this.audioContext.destination);
    this.isCapturing = true;

    console.log('[AudioCapture] Frame capture started');
    return true;
  }

  /**
   * Convert Float32 audio samples to Int16 for Porcupine
   * @private
   */
  _float32ToInt16(float32Array) {
    const int16Array = new Int16Array(float32Array.length);
    for (let i = 0; i < float32Array.length; i++) {
      const s = Math.max(-1, Math.min(1, float32Array[i]));
      int16Array[i] = s < 0 ? s * 0x8000 : s * 0x7FFF;
    }
    return int16Array;
  }

  /**
   * Get current audio level (0-1) for visualization
   * @returns {number} RMS audio level
   */
  getAudioLevel() {
    if (!this.analyser || !this.dataArray) return 0;
    this.analyser.getByteTimeDomainData(this.dataArray);
    let sum = 0;
    for (let i = 0; i < this.dataArray.length; i++) {
      const normalized = (this.dataArray[i] - 128) / 128;
      sum += normalized * normalized;
    }
    return Math.sqrt(sum / this.dataArray.length);
  }

  /**
   * Setup analyser for audio visualization
   */
  setupAnalyser() {
    if (!this.audioContext || !this.mediaStream) return;

    this.analyser = this.audioContext.createAnalyser();
    this.analyser.fftSize = 256;
    this.dataArray = new Uint8Array(this.analyser.frequencyBinCount);

    const source = this.audioContext.createMediaStreamSource(this.mediaStream);
    source.connect(this.analyser);
  }

  /**
   * Start recording audio for Whisper transcription
   */
  startRecording() {
    if (!this.mediaStream) {
      console.error('[AudioCapture] No media stream for recording');
      return false;
    }

    this.audioChunks = [];
    this.mediaRecorder = new MediaRecorder(this.mediaStream, {
      mimeType: 'audio/webm;codecs=opus',
    });

    this.mediaRecorder.ondataavailable = (event) => {
      if (event.data.size > 0) {
        this.audioChunks.push(event.data);
      }
    };

    this.mediaRecorder.start(100); // Collect in 100ms chunks
    console.log('[AudioCapture] Recording started');
    return true;
  }

  /**
   * Stop recording and get audio blob
   * @returns {Promise<Blob>} Audio blob in WebM format
   */
  stopRecording() {
    return new Promise((resolve) => {
      if (!this.mediaRecorder || this.mediaRecorder.state === 'inactive') {
        resolve(new Blob([]));
        return;
      }

      this.mediaRecorder.onstop = () => {
        const blob = new Blob(this.audioChunks, { type: 'audio/webm' });
        console.log('[AudioCapture] Recording stopped:', blob.size, 'bytes');
        resolve(blob);
      };

      this.mediaRecorder.stop();
    });
  }

  /**
   * Stop frame capture
   */
  stopFrameCapture() {
    if (this.processor) {
      this.processor.disconnect();
      this.processor = null;
    }
    if (this.audioContext) {
      this.audioContext.close();
      this.audioContext = null;
    }
    this.onAudioFrame = null;
    this.isCapturing = false;
    console.log('[AudioCapture] Frame capture stopped');
  }

  /**
   * Release all resources
   */
  release() {
    this.stopFrameCapture();
    if (this.mediaStream) {
      this.mediaStream.getTracks().forEach(track => track.stop());
      this.mediaStream = null;
    }
    console.log('[AudioCapture] Released');
  }
}

module.exports = { AudioCapture };
```
  </action>
  <verify>
1. File exists: `src/classes/audioCapture.class.js`
2. Contains `navigator.mediaDevices.getUserMedia`
3. Contains `startFrameCapture`, `startRecording`, `stopRecording` methods
  </verify>
  <done>AudioCapture class created for microphone access and frame extraction</done>
</task>

<task type="auto">
  <name>Task 3: Create Voice Controller Class</name>
  <files>src/classes/voiceController.class.js</files>
  <action>
Create `src/classes/voiceController.class.js`:

```javascript
/**
 * Voice Controller
 * Orchestrates wake word detection, recording, and transcription
 *
 * Context decisions implemented:
 * - 60 second maximum recording duration
 * - Space key cancels listening mode
 * - Wake word ignored during recording (re-triggering disabled)
 */

const { AudioCapture } = require('./audioCapture.class');

// Voice states
const VoiceState = {
  DISABLED: 'disabled',   // Voice not available or not initialized
  IDLE: 'idle',           // Ready but not listening
  LISTENING: 'listening', // Listening for wake word
  RECORDING: 'recording', // Recording audio after wake word
  PROCESSING: 'processing', // Whisper API processing
  ERROR: 'error',         // Error state
};

class VoiceController {
  constructor(options = {}) {
    // Max duration: 60 seconds (from CONTEXT.md)
    this.maxRecordingMs = options.maxRecordingMs || 60000;
    this.silenceTimeoutMs = options.silenceTimeoutMs || 2000;

    // Callbacks
    this.onStateChange = options.onStateChange || (() => {});
    this.onTranscription = options.onTranscription || (() => {});
    this.onError = options.onError || (() => {});
    this.onWakeDetected = options.onWakeDetected || (() => {});
    this.onAudioLevel = options.onAudioLevel || (() => {}); // For waveform viz
    this.onInterimTranscription = options.onInterimTranscription || (() => {}); // For interim results

    this.audioCapture = new AudioCapture();

    this.state = VoiceState.DISABLED;
    this.silenceTimer = null;
    this.maxDurationTimer = null;
    this.audioLevelInterval = null;
    this.isInitialized = false;
    this.isEnabled = false; // Voice toggle state

    // Bind space key handler
    this._boundKeyHandler = this._handleKeyDown.bind(this);
  }

  /**
   * Initialize voice controller
   * @returns {Promise<boolean>} True if initialization successful
   */
  async initialize() {
    try {
      // Check voice availability via IPC
      const availability = await window.ipc.invoke('voice:check-availability');
      if (!availability.available) {
        console.warn('[VoiceController] Voice not available:', availability);
        this._setState(VoiceState.DISABLED);
        return false;
      }

      // Request microphone permission
      const hasPermission = await this.audioCapture.requestPermission();
      if (!hasPermission) {
        this.onError('Microphone permission denied');
        this._setState(VoiceState.ERROR);
        return false;
      }

      // Initialize voice services in main process
      const result = await window.ipc.invoke('voice:initialize');
      if (!result.success) {
        this.onError(result.error || 'Voice initialization failed');
        this._setState(VoiceState.ERROR);
        return false;
      }

      // Setup wake word detection listener
      window.ipc.on('voice:wake-word-detected', () => {
        this._onWakeWordDetected();
      });

      // Add global key handler for space key cancel
      document.addEventListener('keydown', this._boundKeyHandler);

      this.isInitialized = true;
      this._setState(VoiceState.IDLE);
      console.log('[VoiceController] Initialized');
      return true;
    } catch (error) {
      console.error('[VoiceController] Initialization failed:', error.message);
      this.onError(error.message);
      this._setState(VoiceState.ERROR);
      return false;
    }
  }

  /**
   * Handle keydown events - Space cancels recording
   * @private
   */
  _handleKeyDown(event) {
    // Space key cancels listening/recording mode
    if (event.code === 'Space' && (this.state === VoiceState.LISTENING || this.state === VoiceState.RECORDING)) {
      event.preventDefault();
      console.log('[VoiceController] Space key pressed - cancelling');
      this.cancelRecording();
    }
  }

  /**
   * Enable voice listening (toggle on)
   */
  enable() {
    if (!this.isInitialized) {
      console.warn('[VoiceController] Not initialized, cannot enable');
      return false;
    }
    this.isEnabled = true;
    return this.startListening();
  }

  /**
   * Disable voice listening (toggle off)
   */
  disable() {
    this.isEnabled = false;
    this.stopListening();
    this._setState(VoiceState.IDLE);
  }

  /**
   * Toggle voice on/off
   * @returns {boolean} New enabled state
   */
  toggle() {
    if (this.isEnabled) {
      this.disable();
    } else {
      this.enable();
    }
    return this.isEnabled;
  }

  /**
   * Start listening for wake word
   */
  startListening() {
    if (!this.isInitialized) {
      console.warn('[VoiceController] Not initialized');
      return false;
    }

    if (!this.isEnabled) {
      console.warn('[VoiceController] Voice is disabled');
      return false;
    }

    if (this.state === VoiceState.RECORDING || this.state === VoiceState.PROCESSING) {
      console.warn('[VoiceController] Cannot start listening from state:', this.state);
      return false;
    }

    // Start sending audio frames to main process for wake word detection
    this.audioCapture.startFrameCapture((frame) => {
      // Convert Int16Array to regular array for IPC
      window.ipc.send('voice:audio-frame', Array.from(frame));
    });

    this._setState(VoiceState.LISTENING);
    console.log('[VoiceController] Listening for wake word...');
    return true;
  }

  /**
   * Stop listening (but don't disable)
   */
  stopListening() {
    this._clearTimers();
    this._stopAudioLevelPolling();
    this.audioCapture.stopFrameCapture();

    if (this.state === VoiceState.RECORDING) {
      this.audioCapture.stopRecording();
    }

    if (this.isEnabled && this.isInitialized) {
      this._setState(VoiceState.LISTENING);
      // Restart listening for wake word
      this.audioCapture.startFrameCapture((frame) => {
        window.ipc.send('voice:audio-frame', Array.from(frame));
      });
    } else {
      this._setState(VoiceState.IDLE);
    }

    console.log('[VoiceController] Stopped listening');
  }

  /**
   * Cancel current recording without transcribing
   */
  cancelRecording() {
    console.log('[VoiceController] Recording cancelled');
    this._clearTimers();
    this._stopAudioLevelPolling();

    if (this.state === VoiceState.RECORDING) {
      this.audioCapture.stopRecording(); // Discard audio
    }

    // Return to listening if enabled
    if (this.isEnabled) {
      this._setState(VoiceState.LISTENING);
      this.audioCapture.startFrameCapture((frame) => {
        window.ipc.send('voice:audio-frame', Array.from(frame));
      });
    } else {
      this._setState(VoiceState.IDLE);
    }
  }

  /**
   * Handle wake word detection
   * @private
   */
  _onWakeWordDetected() {
    // Ignore wake word during recording (per CONTEXT.md)
    if (this.state !== VoiceState.LISTENING) {
      return;
    }

    console.log('[VoiceController] Wake word detected!');

    // Notify listeners (for audio feedback)
    this.onWakeDetected();

    // Start recording for transcription
    this._setState(VoiceState.RECORDING);
    this.audioCapture.startRecording();

    // Setup analyser for waveform visualization
    this.audioCapture.setupAnalyser();
    this._startAudioLevelPolling();

    // Start silence timeout (user can still speak)
    this._startSilenceTimer();

    // Start max duration timer (60 seconds per CONTEXT.md)
    this._startMaxDurationTimer();
  }

  /**
   * Start polling audio levels for waveform visualization
   * @private
   */
  _startAudioLevelPolling() {
    this._stopAudioLevelPolling();
    this.audioLevelInterval = setInterval(() => {
      const level = this.audioCapture.getAudioLevel();
      this.onAudioLevel(level);
    }, 50); // 20fps
  }

  /**
   * Stop audio level polling
   * @private
   */
  _stopAudioLevelPolling() {
    if (this.audioLevelInterval) {
      clearInterval(this.audioLevelInterval);
      this.audioLevelInterval = null;
    }
  }

  /**
   * Start silence timeout
   * @private
   */
  _startSilenceTimer() {
    this._clearSilenceTimer();
    this.silenceTimer = setTimeout(() => {
      this._onSilenceTimeout();
    }, this.silenceTimeoutMs);
  }

  /**
   * Clear silence timeout
   * @private
   */
  _clearSilenceTimer() {
    if (this.silenceTimer) {
      clearTimeout(this.silenceTimer);
      this.silenceTimer = null;
    }
  }

  /**
   * Start max duration timer (60 seconds)
   * @private
   */
  _startMaxDurationTimer() {
    this._clearMaxDurationTimer();
    this.maxDurationTimer = setTimeout(() => {
      console.log('[VoiceController] Max duration reached (60s)');
      this._onSilenceTimeout(); // Same behavior as silence timeout
    }, this.maxRecordingMs);
  }

  /**
   * Clear max duration timer
   * @private
   */
  _clearMaxDurationTimer() {
    if (this.maxDurationTimer) {
      clearTimeout(this.maxDurationTimer);
      this.maxDurationTimer = null;
    }
  }

  /**
   * Clear all timers
   * @private
   */
  _clearTimers() {
    this._clearSilenceTimer();
    this._clearMaxDurationTimer();
  }

  /**
   * Handle silence/max duration timeout - stop recording and transcribe
   * @private
   */
  async _onSilenceTimeout() {
    if (this.state !== VoiceState.RECORDING) {
      return;
    }

    console.log('[VoiceController] Timeout, processing...');
    this._clearTimers();
    this._stopAudioLevelPolling();
    this._setState(VoiceState.PROCESSING);

    try {
      // Stop recording and get audio
      const audioBlob = await this.audioCapture.stopRecording();

      if (audioBlob.size === 0) {
        console.warn('[VoiceController] No audio captured');
        this.onError('No audio captured');
        this._returnToListening();
        return;
      }

      // Convert blob to array buffer for IPC
      const arrayBuffer = await audioBlob.arrayBuffer();
      const audioData = Array.from(new Uint8Array(arrayBuffer));

      // Send to Whisper for transcription
      const result = await window.ipc.invoke('voice:transcribe', audioData);

      if (result.success && result.text) {
        console.log('[VoiceController] Transcription:', result.text);
        this.onTranscription(result.text, true); // true = success
      } else {
        console.warn('[VoiceController] Transcription failed:', result.error);
        this.onTranscription(null, false); // false = failure
        this.onError(result.error || 'Transcription failed');
      }
    } catch (error) {
      console.error('[VoiceController] Processing error:', error.message);
      this.onTranscription(null, false);
      this.onError(error.message);
    }

    this._returnToListening();
  }

  /**
   * Return to listening state if enabled
   * @private
   */
  _returnToListening() {
    if (this.isEnabled) {
      this._setState(VoiceState.LISTENING);
      this.audioCapture.startFrameCapture((frame) => {
        window.ipc.send('voice:audio-frame', Array.from(frame));
      });
    } else {
      this._setState(VoiceState.IDLE);
    }
  }

  /**
   * Update state and notify listeners
   * @private
   */
  _setState(newState) {
    const oldState = this.state;
    this.state = newState;
    if (oldState !== newState) {
      console.log('[VoiceController] State:', oldState, '->', newState);
      this.onStateChange(newState, oldState);
    }
  }

  /**
   * Get current state
   */
  getState() {
    return this.state;
  }

  /**
   * Check if voice is enabled
   */
  getEnabled() {
    return this.isEnabled;
  }

  /**
   * Release all resources
   */
  release() {
    this._clearTimers();
    this._stopAudioLevelPolling();
    document.removeEventListener('keydown', this._boundKeyHandler);
    this.audioCapture.release();
    window.ipc.send('voice:release');
    this._setState(VoiceState.DISABLED);
    this.isInitialized = false;
    this.isEnabled = false;
    console.log('[VoiceController] Released');
  }
}

module.exports = { VoiceController, VoiceState };
```
  </action>
  <verify>
1. File exists: `src/classes/voiceController.class.js`
2. Contains `maxRecordingMs` for 60s timeout
3. Contains `_handleKeyDown` for space key cancel
4. Contains `VoiceState` enum with DISABLED, IDLE, LISTENING, RECORDING, PROCESSING, ERROR
5. Contains `onInterimTranscription` callback placeholder for Plan 03
  </verify>
  <done>VoiceController class created with state machine, 60s timeout, and space key cancel</done>
</task>

</tasks>

<verification>
After all tasks complete:

1. **Boot integration:**
   ```bash
   grep -n "setupVoiceIPC" src/_boot.js
   grep -n "cleanupVoiceIPC" src/_boot.js
   ```

2. **Classes created:**
   - `src/classes/audioCapture.class.js`
   - `src/classes/voiceController.class.js`

3. **State machine states:**
   - DISABLED, IDLE, LISTENING, RECORDING, PROCESSING, ERROR

4. **Context decisions implemented:**
   - 60 second max recording: `maxRecordingMs: 60000`
   - Space key cancel: `_handleKeyDown` with `event.code === 'Space'`
   - Wake word ignored during recording: check in `_onWakeWordDetected`
</verification>

<success_criteria>
1. Voice IPC handlers initialized on app startup
2. Cleanup runs on app quit
3. AudioCapture requests microphone permission
4. AudioCapture streams frames to main process via IPC
5. VoiceController state machine manages flow correctly
6. 60-second max duration timer implemented
7. Space key cancels recording
</success_criteria>

<output>
After completion, create `.planning/phases/09-voice-foundation/09-02-SUMMARY.md`
</output>
