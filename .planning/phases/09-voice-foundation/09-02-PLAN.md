---
phase: 09-voice-foundation
plan: 02
type: execute
wave: 2
depends_on: ["09-01"]
files_modified:
  - src/_renderer.js
  - src/assets/css/mod_voice.css
  - src/classes/audioFeedback.class.js
  - src/classes/waveformVisualizer.class.js
  - src/classes/voiceToggleWidget.class.js
files_created:
  - src/assets/css/mod_voice.css
  - src/classes/audioFeedback.class.js
  - src/classes/waveformVisualizer.class.js
  - src/classes/voiceToggleWidget.class.js
  - resources/sounds/yes-sir.mp3
  - resources/sounds/success.mp3
  - resources/sounds/failure.mp3
autonomous: false
user_setup:
  - service: audio_files
    why: "Voice feedback requires audio files"
    dashboard_config:
      - task: "Create or download 'Yes sir' voice response (TTS or recorded)"
        location: "Save to resources/sounds/yes-sir.mp3"
      - task: "Create or download success chime sound"
        location: "Save to resources/sounds/success.mp3"
      - task: "Create or download failure/error sound"
        location: "Save to resources/sounds/failure.mp3"

must_haves:
  truths:
    - "Voice toggle button visible on RIGHT SIDE of screen"
    - "'Yes sir' voice response plays on wake word detection"
    - "Distinct success sound plays on transcription success"
    - "Distinct failure sound plays on transcription failure"
    - "Waveform visualization appears at BOTTOM of active terminal during recording"
    - "Interim transcription appears in real-time as user speaks"
    - "Final transcription appears in terminal input line (not floating widget)"
    - "Voice toggle button enables/disables voice listening"
  artifacts:
    - path: "src/classes/audioFeedback.class.js"
      provides: "Audio playback for Yes sir, success, failure sounds"
      contains: "playYesSir"
    - path: "src/classes/waveformVisualizer.class.js"
      provides: "Real-time audio waveform display"
      contains: "class WaveformVisualizer"
    - path: "src/classes/voiceToggleWidget.class.js"
      provides: "Voice enable/disable toggle button"
      contains: "class VoiceToggleWidget"
    - path: "src/assets/css/mod_voice.css"
      provides: "Styling for voice UI components"
      contains: ".voice-toggle"
  key_links:
    - from: "voiceController.class.js"
      to: "audioFeedback.class.js"
      via: "onWakeDetected callback"
      pattern: "playYesSir"
    - from: "voiceController.class.js"
      to: "waveformVisualizer.class.js"
      via: "onAudioLevel callback"
      pattern: "updateLevel"
    - from: "voiceController.class.js"
      to: "terminal input"
      via: "onTranscription callback"
      pattern: "terminalInput\\.value"
---

<objective>
Implement voice UX polish: "Yes sir" voice response, success/failure sounds, waveform visualization at terminal bottom, voice toggle button, and terminal input integration.

Purpose: Transform the core voice infrastructure (Plan 01) into a polished user experience that matches the sci-fi terminal aesthetic. All 8 context decisions from 09-CONTEXT.md are addressed across Plan 01 and Plan 02.

Output: Complete voice UX with toggle button, "Yes sir" confirmation, waveform visualization during recording, and transcription appearing directly in terminal input.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/09-voice-foundation/09-CONTEXT.md
@.planning/phases/09-voice-foundation/09-01-SUMMARY.md
</context>

<tasks>

<task type="checkpoint:human-action" gate="blocking">
  <name>Task 0: Create Audio Files for Voice Feedback</name>
  <files>resources/sounds/yes-sir.mp3, resources/sounds/success.mp3, resources/sounds/failure.mp3</files>
  <action>
**User must create or obtain these audio files:**

1. **"Yes sir" Voice Response** (required)
   - Create using TTS (text-to-speech) service OR record yourself
   - Suggested TTS services:
     - ElevenLabs (free tier): https://elevenlabs.io/
     - Google Cloud TTS: https://cloud.google.com/text-to-speech
     - macOS: `say "Yes sir" -o yes-sir.aiff && ffmpeg -i yes-sir.aiff yes-sir.mp3`
   - Duration: 0.5-1 second
   - Save to: `resources/sounds/yes-sir.mp3`

2. **Success Sound** (required)
   - Short positive chime/beep indicating transcription success
   - Free sources:
     - https://freesound.org/ (search "success chime")
     - https://mixkit.co/free-sound-effects/
   - Duration: 0.3-0.5 seconds
   - Save to: `resources/sounds/success.mp3`

3. **Failure Sound** (required)
   - Short negative sound indicating transcription failure
   - Free sources: Same as above (search "error beep")
   - Duration: 0.3-0.5 seconds
   - Save to: `resources/sounds/failure.mp3`

**Directory creation:**
```bash
mkdir -p resources/sounds
```
  </action>
  <verify>
1. File exists: `resources/sounds/yes-sir.mp3`
2. File exists: `resources/sounds/success.mp3`
3. File exists: `resources/sounds/failure.mp3`
4. All files are playable MP3 audio
  </verify>
  <done>Audio files created/downloaded for voice feedback</done>
  <resume-signal>Type "done" when audio files are ready</resume-signal>
</task>

<task type="auto">
  <name>Task 1: Create Audio Feedback Class with Yes Sir and Success/Failure Sounds</name>
  <files>src/classes/audioFeedback.class.js</files>
  <action>
Create `src/classes/audioFeedback.class.js`:

```javascript
/**
 * Audio Feedback Service
 * Handles voice responses and audio cues for voice interaction
 *
 * Context decisions implemented:
 * - "Yes sir" voice response on wake word detection (not generic chime)
 * - Distinct success sound on transcription success
 * - Distinct failure sound on transcription failure
 */

const path = require('path');
const { Howl, Howler } = require('howler');

class AudioFeedback {
  constructor() {
    this.yesSir = null;
    this.success = null;
    this.failure = null;
    this.isLoaded = false;
  }

  /**
   * Initialize audio feedback with preloaded sounds
   * @returns {boolean} True if initialization successful
   */
  initialize() {
    try {
      const soundsDir = path.join(__dirname, '..', 'resources', 'sounds');

      // "Yes sir" voice response for wake word detection
      this.yesSir = new Howl({
        src: [path.join(soundsDir, 'yes-sir.mp3')],
        volume: 0.7,
        preload: true,
        onload: () => console.log('[AudioFeedback] Yes sir loaded'),
        onloaderror: (id, err) => console.warn('[AudioFeedback] Yes sir load error:', err),
      });

      // Success sound for transcription completion
      this.success = new Howl({
        src: [path.join(soundsDir, 'success.mp3')],
        volume: 0.5,
        preload: true,
        onload: () => console.log('[AudioFeedback] Success sound loaded'),
        onloaderror: (id, err) => console.warn('[AudioFeedback] Success load error:', err),
      });

      // Failure sound for transcription errors
      this.failure = new Howl({
        src: [path.join(soundsDir, 'failure.mp3')],
        volume: 0.5,
        preload: true,
        onload: () => console.log('[AudioFeedback] Failure sound loaded'),
        onloaderror: (id, err) => console.warn('[AudioFeedback] Failure load error:', err),
      });

      // Respect global audio settings
      if (window.settings && window.settings.audio === false) {
        Howler.volume(0);
      } else if (window.settings && typeof window.settings.audioVolume === 'number') {
        Howler.volume(window.settings.audioVolume);
      }

      this.isLoaded = true;
      console.log('[AudioFeedback] Initialized');
      return true;
    } catch (error) {
      console.error('[AudioFeedback] Initialization failed:', error.message);
      return false;
    }
  }

  /**
   * Play "Yes sir" voice response on wake word detection
   * Per CONTEXT.md: Voice response, not generic chime
   */
  playYesSir() {
    if (!this.yesSir) {
      console.warn('[AudioFeedback] Yes sir not loaded');
      return;
    }

    try {
      this.yesSir.play();
      console.log('[AudioFeedback] Playing: Yes sir');
    } catch (error) {
      console.warn('[AudioFeedback] Yes sir play error:', error.message);
    }
  }

  /**
   * Play success sound on transcription success
   */
  playSuccess() {
    if (!this.success) {
      console.warn('[AudioFeedback] Success sound not loaded');
      return;
    }

    try {
      this.success.play();
      console.log('[AudioFeedback] Playing: Success');
    } catch (error) {
      console.warn('[AudioFeedback] Success play error:', error.message);
    }
  }

  /**
   * Play failure sound on transcription failure
   */
  playFailure() {
    if (!this.failure) {
      console.warn('[AudioFeedback] Failure sound not loaded');
      return;
    }

    try {
      this.failure.play();
      console.log('[AudioFeedback] Playing: Failure');
    } catch (error) {
      console.warn('[AudioFeedback] Failure play error:', error.message);
    }
  }

  /**
   * Set master volume (0-1)
   * @param {number} volume
   */
  setVolume(volume) {
    Howler.volume(Math.max(0, Math.min(1, volume)));
  }

  /**
   * Release audio resources
   */
  release() {
    if (this.yesSir) {
      this.yesSir.unload();
      this.yesSir = null;
    }
    if (this.success) {
      this.success.unload();
      this.success = null;
    }
    if (this.failure) {
      this.failure.unload();
      this.failure = null;
    }
    this.isLoaded = false;
    console.log('[AudioFeedback] Released');
  }
}

module.exports = { AudioFeedback };
```
  </action>
  <verify>
1. File exists: `src/classes/audioFeedback.class.js`
2. Contains `playYesSir`, `playSuccess`, `playFailure` methods
3. Uses Howler.js (already in project for existing audio)
  </verify>
  <done>AudioFeedback class created with Yes sir, success, and failure sounds</done>
</task>

<task type="auto">
  <name>Task 2: Create Waveform Visualizer Class</name>
  <files>src/classes/waveformVisualizer.class.js</files>
  <action>
Create `src/classes/waveformVisualizer.class.js`:

```javascript
/**
 * Waveform Visualizer
 * Real-time audio waveform display at bottom of active terminal
 *
 * Context decisions implemented:
 * - Waveform visualization showing audio input levels in real-time
 * - Position: Bottom of active terminal (not floating)
 * - Sci-fi terminal aesthetic
 */

class WaveformVisualizer {
  constructor(options = {}) {
    this.barCount = options.barCount || 32;
    this.container = null;
    this.bars = [];
    this.isVisible = false;
    this.targetTerminalIndex = null;
    this.animationFrame = null;
    this.levels = new Array(this.barCount).fill(0);
    this.levelIndex = 0;
  }

  /**
   * Create waveform DOM element
   * @private
   */
  _createDOM() {
    // Create container
    this.container = document.createElement('div');
    this.container.className = 'voice-waveform';
    this.container.innerHTML = `
      <div class="voice-waveform__label">VOICE INPUT</div>
      <div class="voice-waveform__bars"></div>
      <div class="voice-waveform__status">Recording...</div>
    `;

    // Create bars
    const barsContainer = this.container.querySelector('.voice-waveform__bars');
    for (let i = 0; i < this.barCount; i++) {
      const bar = document.createElement('div');
      bar.className = 'voice-waveform__bar';
      barsContainer.appendChild(bar);
      this.bars.push(bar);
    }

    return this.container;
  }

  /**
   * Show waveform at bottom of specified terminal
   * @param {number} terminalIndex - Active terminal index (0-4)
   */
  show(terminalIndex) {
    if (this.isVisible) {
      this.hide();
    }

    this.targetTerminalIndex = terminalIndex;

    // Find the terminal container
    const terminalSelector = `#mod_term_${terminalIndex}`;
    const terminalEl = document.querySelector(terminalSelector);

    if (!terminalEl) {
      // Fallback to active terminal area
      const termWrapper = document.querySelector('.mod_term_wrapper') ||
                          document.querySelector('#mod_terminals');
      if (termWrapper) {
        if (!this.container) {
          this._createDOM();
        }
        termWrapper.appendChild(this.container);
      } else {
        console.warn('[WaveformVisualizer] Cannot find terminal container');
        return;
      }
    } else {
      if (!this.container) {
        this._createDOM();
      }
      terminalEl.appendChild(this.container);
    }

    this.container.classList.add('voice-waveform--visible');
    this.isVisible = true;
    this._startAnimation();

    console.log('[WaveformVisualizer] Shown at terminal', terminalIndex);
  }

  /**
   * Hide waveform
   */
  hide() {
    if (!this.isVisible) return;

    this._stopAnimation();

    if (this.container) {
      this.container.classList.remove('voice-waveform--visible');
      // Remove from DOM after transition
      setTimeout(() => {
        if (this.container && this.container.parentNode) {
          this.container.parentNode.removeChild(this.container);
        }
      }, 300);
    }

    this.isVisible = false;
    this.targetTerminalIndex = null;
    console.log('[WaveformVisualizer] Hidden');
  }

  /**
   * Update with new audio level
   * @param {number} level - Audio level 0-1
   */
  updateLevel(level) {
    // Store level in circular buffer for smoother visualization
    this.levels[this.levelIndex] = level;
    this.levelIndex = (this.levelIndex + 1) % this.barCount;
  }

  /**
   * Update status text
   * @param {string} status - Status message
   */
  setStatus(status) {
    if (this.container) {
      const statusEl = this.container.querySelector('.voice-waveform__status');
      if (statusEl) {
        statusEl.textContent = status;
      }
    }
  }

  /**
   * Start animation loop
   * @private
   */
  _startAnimation() {
    const animate = () => {
      this._renderBars();
      this.animationFrame = requestAnimationFrame(animate);
    };
    animate();
  }

  /**
   * Stop animation loop
   * @private
   */
  _stopAnimation() {
    if (this.animationFrame) {
      cancelAnimationFrame(this.animationFrame);
      this.animationFrame = null;
    }
  }

  /**
   * Render bar heights based on levels
   * @private
   */
  _renderBars() {
    for (let i = 0; i < this.barCount; i++) {
      // Get level with offset for wave effect
      const levelIdx = (this.levelIndex + i) % this.barCount;
      const level = this.levels[levelIdx];

      // Add some randomness for sci-fi effect
      const jitter = Math.random() * 0.1;
      const height = Math.max(0.05, Math.min(1, level + jitter));

      this.bars[i].style.height = `${height * 100}%`;

      // Color intensity based on level
      const intensity = Math.floor(200 + level * 55);
      this.bars[i].style.backgroundColor = `rgb(${intensity}, ${intensity}, ${intensity})`;
    }
  }

  /**
   * Show interim transcription text
   * @param {string} text - Interim transcription
   */
  showInterim(text) {
    if (this.container) {
      const statusEl = this.container.querySelector('.voice-waveform__status');
      if (statusEl && text) {
        statusEl.textContent = text;
        statusEl.classList.add('voice-waveform__status--interim');
      }
    }
  }

  /**
   * Release resources
   */
  release() {
    this.hide();
    this.container = null;
    this.bars = [];
    console.log('[WaveformVisualizer] Released');
  }
}

module.exports = { WaveformVisualizer };
```
  </action>
  <verify>
1. File exists: `src/classes/waveformVisualizer.class.js`
2. Contains `show(terminalIndex)`, `hide()`, `updateLevel(level)` methods
3. Attaches to terminal container (not floating)
  </verify>
  <done>WaveformVisualizer class created for real-time audio display</done>
</task>

<task type="auto">
  <name>Task 3: Create Voice Toggle Widget Class</name>
  <files>src/classes/voiceToggleWidget.class.js</files>
  <action>
Create `src/classes/voiceToggleWidget.class.js`:

```javascript
/**
 * Voice Toggle Widget
 * Button on right side of screen to enable/disable voice listening
 *
 * Context decisions implemented:
 * - UI button on RIGHT SIDE of screen (not settings)
 * - Toggle enables/disables voice listening
 */

class VoiceToggleWidget {
  constructor(voiceController) {
    this.voiceController = voiceController;
    this.element = null;
    this.isEnabled = false;
  }

  /**
   * Create and insert the widget into the DOM
   * @param {Element} container - Parent container (right column)
   */
  create(container) {
    this.element = document.createElement('div');
    this.element.id = 'mod_voiceToggle';
    this.element.className = 'voice-toggle';
    this.element.innerHTML = `
      <div class="voice-toggle__inner">
        <div class="voice-toggle__icon">
          <svg viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2">
            <path d="M12 1a3 3 0 0 0-3 3v8a3 3 0 0 0 6 0V4a3 3 0 0 0-3-3z"/>
            <path d="M19 10v2a7 7 0 0 1-14 0v-2"/>
            <line x1="12" y1="19" x2="12" y2="23"/>
            <line x1="8" y1="23" x2="16" y2="23"/>
          </svg>
        </div>
        <div class="voice-toggle__label">VOICE</div>
        <div class="voice-toggle__status">OFF</div>
      </div>
    `;

    // Add click handler
    this.element.addEventListener('click', () => this._handleClick());

    // Insert into container
    if (container) {
      container.appendChild(this.element);
    }

    // Update visual state based on controller
    this._updateVisualState();

    console.log('[VoiceToggle] Widget created');
    return this.element;
  }

  /**
   * Handle click to toggle voice
   * @private
   */
  _handleClick() {
    if (!this.voiceController) {
      console.warn('[VoiceToggle] No voice controller');
      return;
    }

    const newState = this.voiceController.toggle();
    this.isEnabled = newState;
    this._updateVisualState();

    console.log('[VoiceToggle] Toggled to:', newState ? 'ON' : 'OFF');
  }

  /**
   * Update visual state based on enabled/disabled
   * @private
   */
  _updateVisualState() {
    if (!this.element) return;

    const statusEl = this.element.querySelector('.voice-toggle__status');

    if (this.isEnabled) {
      this.element.classList.add('voice-toggle--active');
      this.element.classList.remove('voice-toggle--inactive');
      if (statusEl) statusEl.textContent = 'ON';
    } else {
      this.element.classList.remove('voice-toggle--active');
      this.element.classList.add('voice-toggle--inactive');
      if (statusEl) statusEl.textContent = 'OFF';
    }
  }

  /**
   * Update state from external source (e.g., voice controller state change)
   * @param {boolean} enabled
   */
  setEnabled(enabled) {
    this.isEnabled = enabled;
    this._updateVisualState();
  }

  /**
   * Show recording state
   */
  showRecording() {
    if (this.element) {
      this.element.classList.add('voice-toggle--recording');
      const statusEl = this.element.querySelector('.voice-toggle__status');
      if (statusEl) statusEl.textContent = 'REC';
    }
  }

  /**
   * Show processing state
   */
  showProcessing() {
    if (this.element) {
      this.element.classList.remove('voice-toggle--recording');
      this.element.classList.add('voice-toggle--processing');
      const statusEl = this.element.querySelector('.voice-toggle__status');
      if (statusEl) statusEl.textContent = '...';
    }
  }

  /**
   * Reset to normal state
   */
  resetState() {
    if (this.element) {
      this.element.classList.remove('voice-toggle--recording', 'voice-toggle--processing');
      this._updateVisualState();
    }
  }

  /**
   * Show unavailable state (voice not configured)
   */
  showUnavailable() {
    if (this.element) {
      this.element.classList.add('voice-toggle--unavailable');
      const statusEl = this.element.querySelector('.voice-toggle__status');
      if (statusEl) statusEl.textContent = 'N/A';
    }
  }

  /**
   * Release resources
   */
  release() {
    if (this.element && this.element.parentNode) {
      this.element.parentNode.removeChild(this.element);
    }
    this.element = null;
    console.log('[VoiceToggle] Released');
  }
}

module.exports = { VoiceToggleWidget };
```
  </action>
  <verify>
1. File exists: `src/classes/voiceToggleWidget.class.js`
2. Contains `create(container)`, `toggle`, `setEnabled` methods
3. Has microphone SVG icon
  </verify>
  <done>VoiceToggleWidget class created for voice enable/disable button</done>
</task>

<task type="auto">
  <name>Task 4: Create Voice CSS Styles</name>
  <files>src/assets/css/mod_voice.css</files>
  <action>
Create `src/assets/css/mod_voice.css`:

```css
/**
 * Voice Module Styles
 * Styling for voice toggle button and waveform visualizer
 */

/* =============================================================================
   Voice Toggle Widget (Right Side)
   ============================================================================= */

.voice-toggle {
  border-top: 0.092vh solid rgba(var(--color_r), var(--color_g), var(--color_b), 0.3);
  font-family: var(--font_main_light);
  letter-spacing: 0.092vh;
  padding: 0.8vh 0;
  cursor: pointer;
  transition: all 0.2s ease;
  user-select: none;
}

.voice-toggle::before {
  content: "";
  border-left: 0.092vh solid rgba(var(--color_r), var(--color_g), var(--color_b), 0.3);
  align-self: flex-start;
  position: relative;
  left: -0.092vh;
  top: -1.111vh;
  height: 0.833vh;
}

.voice-toggle::after {
  content: "";
  border-right: 0.092vh solid rgba(var(--color_r), var(--color_g), var(--color_b), 0.3);
  position: relative;
  right: -0.092vh;
  top: -1.111vh;
  height: 0.833vh;
}

.voice-toggle__inner {
  display: flex;
  flex-direction: column;
  align-items: center;
  padding: 0.5vh;
}

.voice-toggle__icon {
  width: 2.5vh;
  height: 2.5vh;
  color: rgba(var(--color_r), var(--color_g), var(--color_b), 0.5);
  transition: color 0.2s ease;
}

.voice-toggle__icon svg {
  width: 100%;
  height: 100%;
}

.voice-toggle__label {
  font-size: 0.9vh;
  color: rgba(var(--color_r), var(--color_g), var(--color_b), 0.7);
  margin-top: 0.3vh;
}

.voice-toggle__status {
  font-size: 1.2vh;
  font-weight: bold;
  color: rgba(var(--color_r), var(--color_g), var(--color_b), 0.5);
  margin-top: 0.2vh;
}

/* Active state */
.voice-toggle--active .voice-toggle__icon {
  color: rgb(var(--color_r), var(--color_g), var(--color_b));
}

.voice-toggle--active .voice-toggle__status {
  color: #22c55e;
}

/* Inactive state */
.voice-toggle--inactive {
  opacity: 0.6;
}

/* Recording state */
.voice-toggle--recording .voice-toggle__icon {
  color: #ef4444;
  animation: voice-pulse 1s ease-in-out infinite;
}

.voice-toggle--recording .voice-toggle__status {
  color: #ef4444;
}

/* Processing state */
.voice-toggle--processing .voice-toggle__icon {
  color: #eab308;
}

.voice-toggle--processing .voice-toggle__status {
  color: #eab308;
}

/* Unavailable state */
.voice-toggle--unavailable {
  opacity: 0.3;
  cursor: not-allowed;
}

/* Hover state */
.voice-toggle:hover:not(.voice-toggle--unavailable) {
  background: rgba(var(--color_r), var(--color_g), var(--color_b), 0.05);
}

@keyframes voice-pulse {
  0%, 100% { opacity: 1; transform: scale(1); }
  50% { opacity: 0.7; transform: scale(1.1); }
}

/* =============================================================================
   Waveform Visualizer (Terminal Bottom)
   ============================================================================= */

.voice-waveform {
  position: absolute;
  bottom: 0;
  left: 0;
  right: 0;
  height: 6vh;
  background: linear-gradient(
    to top,
    rgba(0, 0, 0, 0.9) 0%,
    rgba(0, 0, 0, 0.7) 50%,
    transparent 100%
  );
  border-top: 1px solid rgba(var(--color_r), var(--color_g), var(--color_b), 0.3);
  display: flex;
  flex-direction: column;
  align-items: center;
  justify-content: center;
  z-index: 100;
  opacity: 0;
  transform: translateY(100%);
  transition: opacity 0.3s ease, transform 0.3s ease;
  pointer-events: none;
}

.voice-waveform--visible {
  opacity: 1;
  transform: translateY(0);
}

.voice-waveform__label {
  font-family: var(--font_main);
  font-size: 0.8vh;
  color: rgba(var(--color_r), var(--color_g), var(--color_b), 0.6);
  letter-spacing: 0.2vh;
  margin-bottom: 0.5vh;
}

.voice-waveform__bars {
  display: flex;
  align-items: flex-end;
  justify-content: center;
  height: 3vh;
  gap: 2px;
}

.voice-waveform__bar {
  width: 4px;
  min-height: 2px;
  background: rgb(var(--color_r), var(--color_g), var(--color_b));
  border-radius: 1px;
  transition: height 0.05s ease-out;
}

.voice-waveform__status {
  font-family: var(--font_mono);
  font-size: 1vh;
  color: rgba(var(--color_r), var(--color_g), var(--color_b), 0.8);
  margin-top: 0.5vh;
  max-width: 80%;
  white-space: nowrap;
  overflow: hidden;
  text-overflow: ellipsis;
}

.voice-waveform__status--interim {
  color: rgba(var(--color_r), var(--color_g), var(--color_b), 0.6);
  font-style: italic;
}

/* =============================================================================
   Terminal Input Voice Integration
   ============================================================================= */

/* Visual indicator that input came from voice */
.term-input--voice-active {
  border-color: rgba(var(--color_r), var(--color_g), var(--color_b), 0.8) !important;
  box-shadow: 0 0 5px rgba(var(--color_r), var(--color_g), var(--color_b), 0.3);
}
```
  </action>
  <verify>
1. File exists: `src/assets/css/mod_voice.css`
2. Contains `.voice-toggle` and `.voice-waveform` classes
3. Includes animations for recording state
  </verify>
  <done>Voice CSS styles created for toggle button and waveform</done>
</task>

<task type="auto">
  <name>Task 5: Integrate Voice UI into Renderer</name>
  <files>src/_renderer.js</files>
  <action>
Modify `src/_renderer.js` to integrate all voice UI components.

**1. Add CSS import (in the HTML head or at top of file where other CSS is loaded):**
Find where other module CSS files are referenced and add:
```html
<link rel="stylesheet" href="assets/css/mod_voice.css">
```

OR if CSS is imported via JavaScript:
```javascript
// Near other CSS requires/imports
require('./assets/css/mod_voice.css');
```

**2. Add requires after other class imports (around line 40-50):**
```javascript
const { VoiceController, VoiceState } = require('./classes/voiceController.class');
const { AudioFeedback } = require('./classes/audioFeedback.class');
const { WaveformVisualizer } = require('./classes/waveformVisualizer.class');
const { VoiceToggleWidget } = require('./classes/voiceToggleWidget.class');
```

**3. Add voice variables and IPC wrapper (after window.claudeState around line 79):**
```javascript
// Voice control instances
window.voiceController = null;
window.audioFeedback = null;
window.waveformVisualizer = null;
window.voiceToggleWidget = null;

// IPC wrapper for voice (if not using electron remote)
window.ipc = {
  invoke: (channel, ...args) => ipc.invoke(channel, ...args),
  send: (channel, ...args) => ipc.send(channel, ...args),
  on: (channel, callback) => ipc.on(channel, (event, ...args) => callback(...args)),
};
```

**4. Add voice initialization function (after other initialization functions):**
```javascript
/**
 * Initialize voice control system
 * Called after DOM is ready and other systems initialized
 */
async function initializeVoice() {
  console.log('[Voice] Initializing voice system...');

  // Create audio feedback handler
  window.audioFeedback = new AudioFeedback();
  window.audioFeedback.initialize();

  // Create waveform visualizer
  window.waveformVisualizer = new WaveformVisualizer({ barCount: 32 });

  // Create voice controller with callbacks
  window.voiceController = new VoiceController({
    maxRecordingMs: 60000, // 60 seconds per CONTEXT.md
    silenceTimeoutMs: window.settings.voiceSilenceTimeout || 2000,

    onStateChange: (state, oldState) => {
      console.log('[Voice] State changed:', oldState, '->', state);

      // Update toggle widget
      if (window.voiceToggleWidget) {
        if (state === VoiceState.RECORDING) {
          window.voiceToggleWidget.showRecording();
        } else if (state === VoiceState.PROCESSING) {
          window.voiceToggleWidget.showProcessing();
        } else {
          window.voiceToggleWidget.resetState();
        }
      }

      // Show/hide waveform
      if (state === VoiceState.RECORDING) {
        const activeTerminal = window.activeTerminal || 0;
        window.waveformVisualizer.show(activeTerminal);
      } else if (oldState === VoiceState.RECORDING) {
        window.waveformVisualizer.hide();
      }
    },

    onWakeDetected: () => {
      // Play "Yes sir" voice response per CONTEXT.md
      window.audioFeedback.playYesSir();
    },

    onTranscription: (text, success) => {
      // Play success/failure sound per CONTEXT.md
      if (success && text) {
        window.audioFeedback.playSuccess();
        // Insert transcription into active terminal input per CONTEXT.md
        insertTranscriptionIntoTerminal(text);
      } else {
        window.audioFeedback.playFailure();
      }
    },

    onAudioLevel: (level) => {
      // Update waveform visualization
      if (window.waveformVisualizer) {
        window.waveformVisualizer.updateLevel(level);
      }
    },

    onError: (error) => {
      console.error('[Voice] Error:', error);
    },
  });

  // Initialize voice controller
  const initialized = await window.voiceController.initialize();

  // Create toggle widget in right column
  const rightColumn = document.querySelector('#mod_column_right');
  if (rightColumn) {
    window.voiceToggleWidget = new VoiceToggleWidget(window.voiceController);
    window.voiceToggleWidget.create(rightColumn);

    if (!initialized) {
      window.voiceToggleWidget.showUnavailable();
    }
  }

  console.log('[Voice] Voice system initialized:', initialized ? 'SUCCESS' : 'UNAVAILABLE');
}

/**
 * Insert transcription text into active terminal input
 * Per CONTEXT.md: Text appears directly in terminal input line
 * @param {string} text - Transcription text
 */
function insertTranscriptionIntoTerminal(text) {
  const activeTerminal = window.activeTerminal || 0;

  // Find the terminal input element
  // This depends on how xterm.js is configured in the project
  // Option 1: If there's a visible input field
  const terminalInput = document.querySelector(`#term_${activeTerminal} input`) ||
                        document.querySelector(`.term-input[data-term="${activeTerminal}"]`);

  if (terminalInput) {
    terminalInput.value = text;
    terminalInput.classList.add('term-input--voice-active');
    terminalInput.focus();

    // Remove highlight after a moment
    setTimeout(() => {
      terminalInput.classList.remove('term-input--voice-active');
    }, 2000);

    console.log('[Voice] Inserted transcription into terminal', activeTerminal);
    return;
  }

  // Option 2: Send directly to xterm.js terminal
  if (window.term && window.term[activeTerminal]) {
    const term = window.term[activeTerminal];
    // Write text to terminal (will appear at cursor)
    term.write(text);
    console.log('[Voice] Wrote transcription to xterm', activeTerminal);
    return;
  }

  console.warn('[Voice] Could not find terminal input for index', activeTerminal);
}
```

**5. Call initializeVoice after DOM ready (find where other modules are initialized):**
Look for where modules like netstat, globe, clock are initialized and add:
```javascript
// Initialize voice system (after other modules)
initializeVoice();
```

This is typically in a DOMContentLoaded handler or after the boot screen completes.
  </action>
  <verify>
1. `grep -n "VoiceController" src/_renderer.js` - shows import and instantiation
2. `grep -n "initializeVoice" src/_renderer.js` - shows function and call
3. `grep -n "audioFeedback" src/_renderer.js` - shows Yes sir integration
4. `grep -n "waveformVisualizer" src/_renderer.js` - shows waveform integration
  </verify>
  <done>Voice UI integrated into renderer with all context decisions</done>
</task>

<task type="auto">
  <name>Task 6: Add Voice CSS to HTML</name>
  <files>src/ui.html</files>
  <action>
Add the voice CSS file to `src/ui.html`.

Find the `<head>` section where other CSS files are linked and add:

```html
<link rel="stylesheet" href="assets/css/mod_voice.css">
```

Add it after other module CSS files like:
- mod_context.css
- mod_todoWidget.css
- mod_agentList.css
  </action>
  <verify>
`grep -n "mod_voice.css" src/ui.html` - should show link tag
  </verify>
  <done>Voice CSS linked in HTML</done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <name>Task 7: Verify Voice UX End-to-End</name>
  <what-built>
Complete voice UX integration:
- Voice toggle button on right side of screen
- "Yes sir" voice response on wake word detection
- Success/failure sounds on transcription result
- Waveform visualization at terminal bottom during recording
- Transcription appearing in terminal input
- 60 second max duration
- Space key cancel
  </what-built>
  <how-to-verify>
1. **Start the app:**
   ```bash
   npm start
   ```

2. **Check toggle button:**
   - Look for VOICE toggle button in right column
   - Click to enable (should show ON, icon brightens)

3. **Test wake word:**
   - Say "Son of Anton" clearly
   - Expected: "Yes sir" voice response plays
   - Expected: Waveform visualization appears at terminal bottom
   - Expected: Toggle shows REC state

4. **Test recording:**
   - Speak a command (e.g., "list all files")
   - Wait 2 seconds for silence timeout
   - Expected: Success chime plays
   - Expected: Text appears in terminal input area

5. **Test space cancel:**
   - Say "Son of Anton" to start recording
   - Press Space key
   - Expected: Recording cancelled, returns to listening

6. **Test failure (optional):**
   - Remove OPENAI_API_KEY temporarily
   - Try voice input
   - Expected: Failure sound plays

7. **Test 60s timeout (optional):**
   - Say wake word, keep talking for 60+ seconds
   - Expected: Auto-stops and processes at 60s mark
  </how-to-verify>
  <resume-signal>Type "approved" if all tests pass, or describe issues found</resume-signal>
</task>

</tasks>

<verification>
After all tasks complete:

1. **Audio files exist:**
   ```bash
   ls resources/sounds/
   # Should show: yes-sir.mp3, success.mp3, failure.mp3
   ```

2. **Classes created:**
   ```bash
   ls src/classes/audio*.class.js src/classes/waveform*.class.js src/classes/voice*.class.js
   ```

3. **CSS created:**
   ```bash
   cat src/assets/css/mod_voice.css | head -20
   ```

4. **Integration complete:**
   ```bash
   grep -n "initializeVoice" src/_renderer.js
   grep -n "mod_voice.css" src/ui.html
   ```

5. **Context decisions verified:**
   - [ ] Voice toggle on RIGHT SIDE: VoiceToggleWidget in #mod_column_right
   - [ ] "Yes sir" response: AudioFeedback.playYesSir() in onWakeDetected
   - [ ] Success/failure sounds: playSuccess/playFailure in onTranscription
   - [ ] 60 second timeout: maxRecordingMs: 60000 in VoiceController
   - [ ] Waveform at terminal bottom: WaveformVisualizer.show(terminalIndex)
   - [ ] Space key cancel: _handleKeyDown in VoiceController
   - [ ] Transcription in terminal input: insertTranscriptionIntoTerminal()
</verification>

<success_criteria>
1. Voice toggle button visible and functional on right side
2. "Yes sir" voice plays on wake word detection (not generic chime)
3. Success sound plays on successful transcription
4. Failure sound plays on transcription error
5. Waveform visualization appears at bottom of active terminal
6. Space key immediately cancels recording
7. Transcription text appears in terminal input line
8. All 8 context decisions from 09-CONTEXT.md implemented
</success_criteria>

<output>
After completion, create `.planning/phases/09-voice-foundation/09-02-SUMMARY.md`
</output>
